{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stavco9/neuralnetworks-exc3/blob/main/a3_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8be8cbc-53d9-4c1b-bde5-17d0d95e930c",
      "metadata": {
        "id": "e8be8cbc-53d9-4c1b-bde5-17d0d95e930c"
      },
      "source": [
        "# Student:\n",
        "## Stav Cohen - 316492776"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a9cc8f0",
      "metadata": {
        "id": "5a9cc8f0"
      },
      "source": [
        "# Assingment 3: Shaikspear Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0ef68b",
      "metadata": {
        "id": "ad0ef68b"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Your task is to build a transformer model from scratch and train it on Shakespeare's works. By the end of this assignment, you'll implement the core concepts behind transformers - the architecture that powers modern language models like GPT, BERT, and many others.\n",
        "\n",
        "### Overview of Transformer Architecture\n",
        "\n",
        "Transformers revolutionized natural language processing by introducing the self-attention mechanism, which allows the model to process all positions in a sequence simultaneously, rather than sequentially like RNNs. The key innovations include:\n",
        "\n",
        "1. **Self-Attention**: Allows each position to attend to all positions in the previous layer\n",
        "2. **Multi-Head Attention**: Runs multiple attention operations in parallel\n",
        "3. **Positional Encoding**: Adds information about the position of tokens in the sequence\n",
        "4. **Feed-Forward Networks**: Processes the attended features\n",
        "5. **Layer Normalization**: Stabilizes training of deep networks\n",
        "6. **Residual Connections**: Helps with gradient flow in deep networks\n",
        "\n",
        "### What We're Building\n",
        "\n",
        "We'll implement a simplified GPT2-style (decoder-only) transformer that can:\n",
        "- Take a sequence of Shakespeare text as input\n",
        "- Predict the next character or word in the sequence\n",
        "- Generate new text in Shakespeare's style\n",
        "\n",
        "Let's begin by setting up our environment and loading the Shakespeare dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "542f4bcb",
      "metadata": {
        "id": "542f4bcb"
      },
      "source": [
        "## Your Tasks\n",
        "\n",
        "Complete all the **TODO** sections in the provided code to build a working transformer model that can generate Shakespeare-style text.\n",
        "\n",
        "### Step-by-Step Instructions\n",
        "\n",
        "#### 1. **Read and Understand First**\n",
        "- **Before coding anything**, carefully read through the entire notebook\n",
        "- Pay special attention to the explanatory sections that describe:\n",
        "  - How transformers work\n",
        "  - The difference between encoder and decoder architectures\n",
        "  - The role of attention mechanisms\n",
        "  - Why we use masking in decoder blocks\n",
        "\n",
        "#### 2. **Complete the TODO Tasks in Order**\n",
        "\n",
        "**TODO 1: Character Tokenizer** (`CharacterTokenizer` class)\n",
        "- Implement character-to-index and index-to-character mappings\n",
        "- Create `encode()` and `decode()` methods\n",
        "- Test your tokenizer works correctly\n",
        "\n",
        "**TODO 2: Dataset Preparation**\n",
        "- Convert text to tensor format\n",
        "- Understand the input-target relationship for next-token prediction\n",
        "\n",
        "**TODO 3: Dataset Class** (`ShakespeareDataset`)\n",
        "- Implement `__getitem__()` method to generate training sequences\n",
        "- Ensure proper input-target shifting (input: \"To be or\", target: \"o be or \")\n",
        "\n",
        "**TODO 4: Multi-Layer Perceptron** (`Mlp` class)\n",
        "- Build a simple feed-forward network\n",
        "- Use GELU activation and proper dimensions (embed_size â†’ 4*embed_size â†’ embed_size)\n",
        "\n",
        "**TODO 5: Self-Attention Block** (`SelfAttentionBlock`)\n",
        "- **This is the most complex part** - implement the complete attention mechanism:\n",
        "  - Generate Query, Key, Value matrices\n",
        "  - Reshape for multi-head processing\n",
        "  - Compute attention scores and apply causal masking\n",
        "  - Apply softmax and combine with values\n",
        "  - Concatenate multi-head results and project output\n",
        "- Follow the detailed step-by-step guide provided in the comments\n",
        "\n",
        "**TODO 6: Transformer Block** (`Block` class)\n",
        "- Combine attention and MLP with residual connections\n",
        "- Apply layer normalization correctly\n",
        "\n",
        "**TODO 7: Full GPT-2 Model** (`GPT2` class)\n",
        "- Implement the complete forward pass\n",
        "- Combine token embeddings, positional embeddings, transformer blocks, and output projection\n",
        "- Handle loss computation for training\n",
        "\n",
        "**TODO 8: Text Generation Pipeline** (`pipeline` function)\n",
        "- Implement top-k sampling for text generation\n",
        "- Handle autoregressive generation (one token at a time)\n",
        "\n",
        "# 3. **Explain your code**\n",
        "- After completing each section, write in a markdown section with an explanation of how your code works and why you made certain design choices\n",
        "\n",
        "\n",
        "### Success Criteria\n",
        "\n",
        "You'll know you're successful when:\n",
        "- [v] All TODO sections are completed without errors\n",
        "- [v] The model trains successfully (loss decreases over time)\n",
        "- [v] Generated text resembles Shakespeare's style (Dont expect perfection, It should be english text with some Shakespearean flair, probably with some nonsense or non coherent phrases)\n",
        "- [v] You can explain how each component works\n",
        "\n",
        "### ðŸ’¡ Learning Goals\n",
        "\n",
        "By the end of this assignment, you should understand:\n",
        "- How transformers process sequential data\n",
        "- The role of attention in capturing relationships between tokens\n",
        "- Why masking is crucial for autoregressive models\n",
        "- How modern language models generate text\n",
        "- The architecture behind models like GPT, Claude, and ChatGPT\n",
        "\n",
        "Remember: This assignment is about understanding, not just completing code. Take your time to read and comprehend each section before implementing the TODOs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6a1d0dbe",
      "metadata": {
        "id": "6a1d0dbe"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "from collections import Counter, deque\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "LOAD_PRETRAINED_MODEL = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8607a766",
      "metadata": {
        "id": "8607a766"
      },
      "source": [
        "## Load  shakespeare.text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "tIAEbY7mRBij",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIAEbY7mRBij",
        "outputId": "d3fe690e-4610-4cd1-a7ca-af12294ba4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1089k  100 1089k    0     0  2183k      0 --:--:-- --:--:-- --:--:-- 2182k\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/stavco9/neuralnetworks-exc3/refs/heads/main/shakespeare.txt -O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3794e0a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3794e0a2",
        "outputId": "b559535e-d047-458b-eb52-31a6fa26029c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n",
            "Number of unique characters: 65\n",
            "First 100 characters:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "f_name = 'shakespeare.txt' # if the file in different location, change this line\n",
        "with open('shakespeare.txt', \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f\"Length of text: {len(text)} characters\")\n",
        "print(f\"Number of unique characters: {len(set(text))}\")\n",
        "print(f\"First 100 characters:\\n{text[:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a73ad71",
      "metadata": {
        "id": "0a73ad71"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84578904",
      "metadata": {
        "id": "84578904"
      },
      "source": [
        "# Dataset Encoding and Tokenization\n",
        "\n",
        "## Understanding Text Encoding for Transformers\n",
        "\n",
        "Before we can feed text into our transformer model, we need to convert it into numerical representations. This process involves:\n",
        "1. **Tokenization**: Breaking text into smaller units (characters, subwords, or words)\n",
        "2. **Vocabulary Building**: Creating a mapping between tokens and unique integers\n",
        "3. **Encoding**: Converting text sequences into sequences of integers\n",
        "4. **Decoding**: Converting integer sequences back to readable text\n",
        "\n",
        "### Character-Level vs Word-Level Tokenization\n",
        "\n",
        "For this assignment, we'll use **character-level tokenization**\n",
        "\n",
        "\n",
        "Let's create a simple character-level tokenizer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d7918b20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7918b20",
        "outputId": "b02d118c-be8d-4fcd-9ef7-b83f2fa89e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocab_size: 65\n",
            "Original: To be or not to be\n",
            "Encoded: [32, 53, 1, 40, 43, 1, 53, 56, 1, 52, 53, 58, 1, 58, 53, 1, 40, 43]\n",
            "Decoded: To be or not to be\n"
          ]
        }
      ],
      "source": [
        "class CharacterTokenizer:\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Initialize tokenizer by building vocabulary from input text.\n",
        "\n",
        "        Args:\n",
        "            text: String containing all training text\n",
        "        \"\"\"\n",
        "        # Get all unique characters and sort them\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        self.chars2integers = {char: idx for idx, char in enumerate(self.chars)}\n",
        "        self.integers2chars = {idx: char for idx, char in enumerate(self.chars)}\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert string to list of integers\"\"\"\n",
        "        return [self.chars2integers[char] for char in text] # Encoder: Take a string, output a list of integers\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of integers back to string\"\"\"\n",
        "        return ''.join([self.integers2chars[idx] for idx in indices]) # Decoder: Take a list of integers, output a string\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the size of the vocabulary\"\"\"\n",
        "        return self.vocab_size\n",
        "\n",
        "# Assuming your text is loaded in a variable called 'text'\n",
        "# Create the tokenizer\n",
        "tokenizer = CharacterTokenizer(text)\n",
        "\n",
        "# Test encoding and decoding\n",
        "sample_text = \"To be or not to be\"\n",
        "encoded = tokenizer.encode(sample_text)\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(\"Tokenizer vocab_size:\", len(tokenizer))\n",
        "print(f\"Original: {sample_text}\")\n",
        "print(f\"Encoded: {encoded}\")\n",
        "print(f\"Decoded: {decoded}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f03695-ded1-427c-b4f4-22d53364434c",
      "metadata": {
        "id": "83f03695-ded1-427c-b4f4-22d53364434c"
      },
      "source": [
        "### TODO 1 - Summary\n",
        "TODO 1 part is responsible for generating the dictionary (Tokenizer) we're going to use during train and inference.\n",
        "The dictionary is built from all the characters in the shakespeare text file, and each char has a bi-directional translation with an integer that describe it (encode <-> decode):\n",
        "\n",
        "Encode: Characters to integers\n",
        "</br>\n",
        "Decode: Integers back to characters\n",
        "\n",
        "We should use integers rather than characters because tensors works with numeric vectors and neural networks require numeric inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a55cc32e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "a55cc32e",
        "outputId": "24edcd08-203a-4703-bbfb-33b1d9cb37a8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot the distribution of character frequencies\n",
        "char_counts = Counter(text)\n",
        "def plot_char_distribution(char_counts):\n",
        "    \"\"\"Plot the distribution of character frequencies.\"\"\"\n",
        "    chars, counts = zip(*char_counts.most_common(30))  # Get top 30 characters\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(chars, counts)\n",
        "    plt.xlabel('Characters')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Character Frequency Distribution')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "plot_char_distribution(char_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "460ab122",
      "metadata": {
        "id": "460ab122"
      },
      "source": [
        "Now let's prepare our dataset for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2827f00c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2827f00c",
        "outputId": "bc99bddb-b2ba-4ed6-bee5-1925a3c8167a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: torch.Size([1115394])\n",
            "Training set size: 1,003,854 characters\n",
            "Validation set size: 111,540 characters\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## TODO: Create a torch tensor from the encoded text\n",
        "data = torch.tensor(tokenizer.encode(text))\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "\n",
        "# Split into train and validation sets (90/10 split)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Training set size: {len(train_data):,} characters\")\n",
        "print(f\"Validation set size: {len(val_data):,} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e2baf7c-f217-4931-aca1-c2a6a7a3841d",
      "metadata": {
        "id": "9e2baf7c-f217-4931-aca1-c2a6a7a3841d"
      },
      "source": [
        "### TODO 2 - Summary\n",
        "In TODO 2 we take our textual dataset (A Shakespeare text file) and encode it into a long tensor. After that, we split the tensor into train (90%) and validation (10%) sets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186d1029",
      "metadata": {
        "id": "186d1029"
      },
      "source": [
        "# Understanding Sequence Generation in Transformers\n",
        "\n",
        "## What is a Transformer Trained to Do?\n",
        "\n",
        "At its core, a transformer language model is trained to perform **next-token prediction**. This simple objective - predicting what comes next - enables the model to generate coherent text, understand language patterns, and even appear to \"understand\" context.\n",
        "\n",
        "### The Autoregressive Nature of Language Modeling\n",
        "\n",
        "Transformers for text generation are **autoregressive**, meaning they generate text one token at a time, using previously generated tokens as context for predicting the next one.\n",
        "\n",
        "```\n",
        "Given: \"To be or not to\"\n",
        "Predict: \"be\" (next token)\n",
        "\n",
        "Given: \"To be or not to be\"  \n",
        "Predict: \",\" (next token)\n",
        "\n",
        "Given: \"To be or not to be,\"\n",
        "Predict: \"that\" (next token)\n",
        "```\n",
        "\n",
        "### Training vs Generation: Two Different Processes\n",
        "\n",
        "#### During Training (Assume not a character-level tokenizer for simplicity....)\n",
        "```python\n",
        "# Training sees the full correct sequence\n",
        "Input:  [\"To\", \"be\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\"]\n",
        "Target: [\"be\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\", \"is\"]\n",
        "\n",
        "# The model learns in parallel:\n",
        "Position 0: Given \"To\" â†’ predict \"be\"\n",
        "Position 1: Given \"To be\" â†’ predict \"or\"\n",
        "Position 2: Given \"To be or\" â†’ predict \"not\"\n",
        "... (all positions simultaneously)\n",
        "```\n",
        "\n",
        "#### During Generation (Autoregressive)\n",
        "```python\n",
        "# Generation builds up token by token\n",
        "Step 1: Input: \"To\" â†’ Model predicts: \"be\"\n",
        "Step 2: Input: \"To be\" â†’ Model predicts: \"or\"  \n",
        "Step 3: Input: \"To be or\" â†’ Model predicts: \"not\"\n",
        "... (sequential generation)\n",
        "```\n",
        "\n",
        "### The Probability Distribution\n",
        "\n",
        "At each step, the transformer doesn't just predict one token - it outputs a **probability distribution** over the entire vocabulary:\n",
        "\n",
        "```python\n",
        "# Example output probabilities after \"To be or not to\"\n",
        "{\n",
        "    \"be\": 0.7,      # High probability - this phrase often repeats\n",
        "    \"die\": 0.1,     # Possible alternative\n",
        "    \"sleep\": 0.05,  # Another thematic alternative\n",
        "    \"live\": 0.03,   # Less likely but possible\n",
        "    \",\": 0.02,      # Could end the phrase here\n",
        "    ... (probabilities for all tokens in vocabulary)\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Context Window and Attention\n",
        "\n",
        "The transformer's **context window** (block_size) determines how much previous text it can \"see\" when making predictions:\n",
        "\n",
        "### What the Model Actually Learns\n",
        "\n",
        "Through next-token prediction, the transformer learns:\n",
        "\n",
        "1. **Syntax and Grammar**\n",
        "   - \"To be or\" â†’ likely followed by \"not\"\n",
        "   - Verb conjugations, sentence structure\n",
        "\n",
        "2. **Semantic Relationships**\n",
        "   - \"king\" often appears near \"queen\", \"throne\", \"crown\"\n",
        "   - Thematic consistency within passages\n",
        "\n",
        "3. **Style and Register**\n",
        "   - Shakespearean vocabulary and phrasing\n",
        "   - Iambic pentameter patterns\n",
        "   - Archaic grammatical structures\n",
        "\n",
        "4. **Long-range Dependencies**\n",
        "   - Rhyme schemes across lines\n",
        "   - Character names and relationships\n",
        "   - Plot consistency\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8054fc",
      "metadata": {
        "id": "8a8054fc"
      },
      "source": [
        "In this assinigment, we train the model to predict the next character given the previous characters:\n",
        "- **Input**: \"To be or\"\n",
        "- **Target**: \"o be or \"\n",
        "In the above exmpale there are 8 trainig examples:\n",
        "1. 'T' -> 'o'\n",
        "2. 'To' -> ' '\n",
        "3. 'To ' -> 'b'\n",
        "4. 'To b' -> 'e'\n",
        "5. 'To be' -> ' '\n",
        "6. 'To be o' -> 'r'\n",
        "8. 'To be or' -> ' '\n",
        "\n",
        "Pay attention that the transformer will process all of them simultaneously, allowing it to learn the relationships between characters and their positions in the sequence. Also notice how the target is shifted by one position. This teaches the model to predict what comes next.\n",
        "\n",
        "Lets create a torch Dataset that generates these input-target pairs from our text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "18c85788",
      "metadata": {
        "id": "18c85788"
      },
      "outputs": [],
      "source": [
        "class ShakespeareDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Shakespeare text data.\n",
        "    Generates overlapping sequences for language modeling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: Tensor of encoded text tokens\n",
        "            block_size: Length of each sequence (context length)\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # Calculate number of possible sequences\n",
        "        self.num_sequences = len(data) - block_size\n",
        "\n",
        "        if self.num_sequences <= 0:\n",
        "            raise ValueError(f\"Data length ({len(data)}) must be greater than block_size ({block_size})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of possible sequences\"\"\"\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sequence and its target.\n",
        "\n",
        "        Args:\n",
        "            idx: Index of the sequence start position\n",
        "\n",
        "        Returns:\n",
        "            tuple: (input_sequence, target_sequence)\n",
        "        \"\"\"\n",
        "        return self.data[idx : idx + self.block_size], self.data[idx + 1 : idx + 1 + self.block_size]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c33da7f",
      "metadata": {
        "id": "4c33da7f"
      },
      "source": [
        "### Example Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f195c043",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f195c043",
        "outputId": "7cd1674b-5b19-4b95-85a7-24d80e37e896"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences available: 1,003,726\n",
            "Validation sequences available: 111,412\n",
            "==================================================\n",
            "Example of a training batch instance:\n",
            "\n",
            "Input:  'ness! serious vanity!\n",
            "Mis-shapen chaos of well-seeming forms!\n",
            "Feather of lead, bright smoke, cold fire,\n",
            "sick health!\n",
            "Still-wakin'\n",
            "==================================================\n",
            "Target: 'ess! serious vanity!\n",
            "Mis-shapen chaos of well-seeming forms!\n",
            "Feather of lead, bright smoke, cold fire,\n",
            "sick health!\n",
            "Still-waking'\n"
          ]
        }
      ],
      "source": [
        "num_workers = 2\n",
        "shuffle_train = True\n",
        "block_size = 128\n",
        "batch_size = 16\n",
        "train_dataset = ShakespeareDataset(train_data, block_size)\n",
        "val_dataset = ShakespeareDataset(val_data, block_size)\n",
        "\n",
        "print(f\"Training sequences available: {len(train_dataset):,}\")\n",
        "print(f\"Validation sequences available: {len(val_dataset):,}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=shuffle_train,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,  # Don't shuffle validation data\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "print(\"Example of a training batch instance:\", end='\\n\\n')\n",
        "print(f\"Input:  '{tokenizer.decode(x[0].tolist())}'\") # Decode the first input sequence\n",
        "print(\"=\" * 50)\n",
        "print(f\"Target: '{tokenizer.decode(y[0].tolist())}'\") # Decode the first target sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cec0666-febe-4929-9ce6-628dc6f34da9",
      "metadata": {
        "id": "2cec0666-febe-4929-9ce6-628dc6f34da9"
      },
      "source": [
        "### TODO 3 - Summary\n",
        "In TODO 3 we build a custom DataLoader class - Which going to be the final input to our neural network\n",
        "\n",
        "The class is a standard class for a textual dataset with next token prediction.\n",
        "\n",
        "Each iteration in the DataLoader is called a \"sequence\", and the size of each sequence is configured under \"batch_size\" parameter.\n",
        "\n",
        "The total number of iterations (the length) of the dataloader is the total dataset length minus the batch size (since each iteration we make a step of exactly one index)\n",
        "\n",
        "In each iteration, as every DataLoader, we get the input (x) and the label / target (y). Here, the input (x) is the text from (curr_index -> curr_index + batch_size), and the target (y) is a \"shift-right\" of the text with exactly one character (curr_sequence + 1 -> curr_sequence + batch_size + 1)\n",
        "\n",
        "We create an example implementation of the DataLoader in order to \"Unit test\" it -> we make a sample of one iteration, and then decode and print both the input (x) and the target (y). We can see that the target is exactly a one character \"shift-right\" of the text\n",
        "\n",
        "The input (x) and target (y) are useful for the training, where the next characters in the target are the ground truth of the Transofmer's next token prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c35756c",
      "metadata": {
        "id": "1c35756c"
      },
      "source": [
        "## Transformer Architecture Overview\n",
        "The transformer is a neural network architecture that revolutionized how AI systems process language. Think of it as a sophisticated pattern-matching system that can understand relationships between words in a sentence, regardless of how far apart they are.\n",
        "\n",
        "**The Core Innovation: Attention**\n",
        "\n",
        "The transformer's key breakthrough is something called \"attention.\" Instead of reading text word-by-word like older systems, transformers look at all words simultaneously and figure out which ones are most relevant to each other. It's like having a conversation where you can instantly consider every word someone has said, not just the most recent ones.\n",
        "\n",
        "**Types of Attention Mechanisms**\n",
        "\n",
        "There are three main types of attention used in transformers:\n",
        "\n",
        "- **Self-Attention(Bidirectional)**: Each position in a sequence attends to all positions in the same sequence. This is like each word asking \"which other words in this sentence are most relevant to understanding me?\"\n",
        "\n",
        "- **Cross-Attention**: Positions in one sequence (like a decoder) attend to positions in a different sequence (like an encoder). This is used in encoder-decoder models where the decoder needs to \"look at\" the encoder's representations while generating output.\n",
        "\n",
        "- **Masked Self-Attention**: A restricted form of self-attention where each position can only attend to previous positions, not future ones. This maintains the causal structure needed for text generation.\n",
        "\n",
        "**How Masking Controls Encoder vs Decoder Behavior**\n",
        "\n",
        "The key difference between encoder and decoder blocks lies in their attention masking:\n",
        "\n",
        "- **Encoder blocks** use **bidirectional self-attention** with no masking. Each token can attend to all other tokens in the sequence, allowing the model to build rich contextual representations by considering the full context.\n",
        "\n",
        "- **Decoder blocks** use **causal masking** (also called autoregressive masking). Each token can only attend to itself and previous tokens, never future ones. This prevents the model from \"cheating\" during training by looking ahead, forcing it to learn to predict the next token based only on what came before.\n",
        "\n",
        "**How It Works**\n",
        "\n",
        "The original transformer has two main parts:\n",
        "- **Encoder**: Takes input text and creates rich representations that capture meaning and context using bidirectional attention\n",
        "- **Decoder**: Uses those representations and generates output text using causal attention plus cross-attention to the encoder\n",
        "\n",
        "Both parts use \"self-attention\" layers that let each word position \"attend to\" or focus on other positions. This creates a web of connections showing which words are most important for understanding each other.\n",
        "\n",
        "**Modern Simplification: Decoder-Only Architecture**\n",
        "\n",
        "However, for many real-world applications, researchers discovered that using only the decoder portion is often sufficient and more efficient. This **decoder-only** approach is what powers popular language models like GPT, Claude, and most modern conversational AI systems. Instead of having separate encoder and decoder stacks, these models use multiple decoder blocks that generate text autoregressively - predicting one token at a time based on all previous tokens using causal masking. This is the architecture we'll be building in this notebook.\n",
        "\n",
        "**Key Components**\n",
        "\n",
        "- **Multi-head attention**: Instead of just one attention mechanism, it runs several in parallel, each potentially focusing on different types of relationships\n",
        "- **Feed-forward networks**: Simple neural networks that process the attention outputs\n",
        "- **Layer normalization and residual connections**: Technical elements that help training stability\n",
        "- **Positional encoding**: Since attention doesn't inherently understand word order, this adds position information\n",
        "\n",
        "**Why It Matters**\n",
        "\n",
        "Transformers can process text in parallel rather than sequentially, making them much faster to train. They're also better at capturing long-range dependencies - understanding how a word at the beginning of a paragraph relates to one at the end. The decoder-only variant proves particularly effective for text generation tasks, as it learns to model the probability distribution of natural language through next-token prediction. The masking mechanism is crucial here - it ensures the model learns proper causal relationships and can generate coherent text without access to future information. This architecture powers modern language models like GPT, BERT, and others that have transformed natural language processing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cef6f9",
      "metadata": {
        "id": "29cef6f9"
      },
      "source": [
        "<img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96452a0",
      "metadata": {
        "id": "c96452a0"
      },
      "source": [
        "### Feed-Forward Network (MLP)\n",
        "Lets start from from simple to complicated. As the diagram shows, the MLP is a simple feed-forward neural network that processes the output of the attention mechanism. In the GPT2 architecture that we are following here it consists of two linear transformations with a GeLU activation in between. The MLp layer streach the dimention of the embedding from `embed_size` to `4*embed_size` and then back to `embed_size`.\n",
        "```python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9efa430",
      "metadata": {
        "id": "a9efa430"
      },
      "source": [
        "### MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7f2be3ca",
      "metadata": {
        "id": "7f2be3ca"
      },
      "outputs": [],
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.embed_size = config.embed_size\n",
        "        self.l1 = nn.Linear(self.embed_size ,4*self.embed_size) # first layer\n",
        "        self.gelu = nn.GELU() # activation function\n",
        "        self.l2 = nn.Linear(4*self.embed_size, self.embed_size) # second layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.gelu(out)\n",
        "        out = self.l2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e66253-33e1-4ece-8af0-b3fb6acf0324",
      "metadata": {
        "id": "74e66253-33e1-4ece-8af0-b3fb6acf0324"
      },
      "source": [
        "## TODO 4 - Summary\n",
        "This part is the MLP (Feed-Forward) Block of the network.\n",
        "\n",
        "The MLP layer is coming after the Self-Attention layer (As we'll see later)\n",
        "\n",
        "This layer is executed per token:\n",
        "* The input is an embedding vector of the token with a size of *embed size*\n",
        "* The first layer is a linear one that extends the embedding to 4-time of the original size\n",
        "* The second layer, also linear, turns back the embedding to it's original size\n",
        "* Between the first and second layer there is a non-linear activation function (Gelu)\n",
        "\n",
        "The MLP part allows the Transformer to:\n",
        "* Learn richer transformations of token representations\n",
        "* Refine or distort information the attention has gathered\n",
        "* Help the model to learn abstract representations about the token (like syntax, semantics, etc.)\n",
        "\n",
        "The embedding size extension makes more parameters and more expressive power, and the Gelu layer found to have better performance and be more probablistic than simple Relu\n",
        "\n",
        "It was a quite straight-forward part with no any major issues"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e9d3a1",
      "metadata": {
        "id": "f7e9d3a1"
      },
      "source": [
        "## Multi Head Masked Self-Attention Implementation\n",
        "\n",
        "The self-attention mechanism is the core of the transformer architecture, allowing the model to weigh the importance of different words in a sequence relative to each other. This guide walks you through completing the provided implementation.\n",
        "\n",
        "### Understanding the Provided Setup\n",
        "\n",
        "You're given a `SelfAttentionBlock` class with the initialization already complete. Here's what's provided:\n",
        "\n",
        "**Linear Layers**:\n",
        "- `self.c_atten`: Creates Q, K, V matrices simultaneously (outputs `3 * head_size * num_heads`)\n",
        "- `self.c_proj`: Final output projection back to embedding size\n",
        "\n",
        "**Causal Mask**:\n",
        "- `self.bias`: Pre-computed lower triangular mask of shape `(1, 1, block_size, block_size)`\n",
        "- Prevents attending to future positions (essential for decoder behavior)\n",
        "\n",
        "**Key Constraint**: `head_size * num_heads = embed_size` (this is verified in the assertion)\n",
        "\n",
        "\n",
        "### Step-by-Step Implementation\n",
        "\n",
        "1. **Input Processing**: Your input tensor `x` has shape `(B, T, C)` where:\n",
        "   - `B` = batch size (number of sequences processed together)\n",
        "   - `T` = sequence length (number of tokens in each sequence)  \n",
        "   - `C` = embedding size (dimension of each token's representation)\n",
        "\n",
        "2. **Generate Q, K, V Matrices**:\n",
        "   - Apply linear transformation: `kqv = self.linear_layer(x)`\n",
        "   - Output shape: `(B, T, 3 * embed_size)`\n",
        "   - Split into three equal parts: `k, q, v = kqv.split(embed_size, dim=2)`\n",
        "   - Each tensor now has shape: `(B, T, embed_size)`\n",
        "\n",
        "3. **Reshape for Multi-Head Attention**: Transform each tensor to separate the attention heads:\n",
        "   - Reshape `k, q, v` tensors: `(B, T, embed_size)` â†’ `(B, T, num_heads, head_size)`\n",
        "   - Transpose dimensions: `(B, T, num_heads, head_size)` â†’ `(B, num_heads, T, head_size)`\n",
        "   - This allows processing all heads in parallel\n",
        "\n",
        "4. **Compute Attention Scores**: Calculate how much each position should attend to others:\n",
        "   - Matrix multiply queries and keys: `scores = q @ k.transpose(-2, -1)`\n",
        "   - Scale by head dimension: `scores = scores / sqrt(head_size)`\n",
        "   - Result shape: `(B, num_heads, T, T)` - each position has scores for all positions\n",
        "\n",
        "5. **Apply Causal Masking**: Prevent attending to future positions:\n",
        "   - Set future positions to negative infinity: `scores.masked_fill(self.mask == 0, float('-inf'))`\n",
        "   - This ensures softmax gives zero probability to future positions\n",
        "\n",
        "6. **Convert to Probabilities**: Apply softmax to get attention weights:\n",
        "   - `attention_weights = softmax(scores, dim=-1)`\n",
        "   - Shape remains: `(B, num_heads, T, T)`\n",
        "   - Each row now sums to 1.0\n",
        "\n",
        "7. **Apply Attention to Values**: Use weights to combine value vectors:\n",
        "   - `output = attention_weights @ v`\n",
        "   - Output shape: `(B, num_heads, T, head_size)`\n",
        "\n",
        "8. **Concatenate Multi-Head Results**: Combine all attention heads:\n",
        "   - Transpose back: `(B, num_heads, T, head_size)` â†’ `(B, T, num_heads, head_size)`\n",
        "   - Reshape to flatten heads: `(B, T, num_heads, head_size)` â†’ `(B, T, embed_size)`\n",
        "   - Ensure tensor is contiguous in memory before reshaping\n",
        "\n",
        "9. **Final Output Projection**: Apply final linear transformation:\n",
        "   - `final_output = self.output_projection(concatenated_output)`\n",
        "   - Input and output both have shape: `(B, T, embed_size)`\n",
        "\n",
        "### Key Implementation Details\n",
        "\n",
        "**Dimension Tracking**: Always verify your tensor shapes at each step. The most common bugs come from incorrect reshaping or matrix multiplication dimensions.\n",
        "\n",
        "**Memory Layout**: Use `.contiguous()` before reshaping operations to ensure proper memory layout.\n",
        "\n",
        "**Masking Strategy**: Pre-compute your causal mask during initialization and store it as a buffer to avoid recomputing it every forward pass.\n",
        "\n",
        "**Scaling Factor**: The `1/sqrt(head_size)` scaling prevents attention scores from becoming too large, which would make gradients vanish after softmax.\n",
        "\n",
        "**Efficient Computation**: Modern frameworks often provide optimized attention functions (like `scaled_dot_product_attention`) that handle steps 4-7 efficiently, but understanding the manual implementation helps you debug and customize the mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f163eba3",
      "metadata": {
        "id": "f163eba3"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(SelfAttentionBlock, self).__init__()\n",
        "        assert config.embed_size % config.num_heads == 0\n",
        "\n",
        "        # self-attention values (key, query, value). pay attention that head_size * num_heads must be equal to embed_size\n",
        "        self.c_atten = nn.Linear(config.embed_size, 3*config.embed_size) # multiply by 3 to get k, q, v\n",
        "\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.embed_size, config.embed_size)\n",
        "\n",
        "        self.embed_size = config.embed_size\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_size = int(config.embed_size / config.num_heads)\n",
        "\n",
        "        self.device = config.device\n",
        "\n",
        "        mask = torch.tril(torch.ones(config.block_size, config.block_size)) # Lower triangular mask for causal attention\n",
        "        mask = mask.view(1, 1, config.block_size, config.block_size) # B, 1, T, T\n",
        "        self.register_buffer('mask', mask)  # register_buffer allows the mask to be part of the model state but not a parameter to optimize\n",
        "\n",
        "    def forward(self, x):\n",
        "        #  after each line print the shape of x\n",
        "        \"\"\"        Forward pass for the self-attention block.\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, T, C) where B is batch size, T is sequence length, C is embedding size\n",
        "        Returns:\n",
        "            out: Output tensor of shape (B, T, C) after self-attention and projection\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Input processing\n",
        "        B, T, C = x.shape # Batch, sequence length, embed_size\n",
        "\n",
        "        # 2. Generate Q, K, V Matrices\n",
        "        kqv = self.c_atten(x)\n",
        "        assert kqv.shape == (B, T, 3 * self.embed_size)\n",
        "        k, q, v = kqv.split(self.embed_size, dim=2)\n",
        "        assert k.shape == (B, T, self.embed_size)\n",
        "        assert q.shape == (B, T, self.embed_size)\n",
        "        assert v.shape == (B, T, self.embed_size)\n",
        "\n",
        "        # 3. Reshape for Multi-Head Attention\n",
        "        k = k.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        q = q.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        assert k.shape == (B, self.num_heads, T, self.head_size)\n",
        "        assert q.shape == (B, self.num_heads, T, self.head_size)\n",
        "        assert v.shape == (B, self.num_heads, T, self.head_size)\n",
        "\n",
        "        # 4. Compute Attention Scores\n",
        "        scores = q @ k.transpose(-2, -1)\n",
        "        scale = torch.tensor(self.head_size).to(self.device).sqrt()\n",
        "        scores = scores / scale\n",
        "        assert scores.shape == (B, self.num_heads, T, T)\n",
        "\n",
        "        # 5. Apply Causal Masking\n",
        "        scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        # 6. Convert to Probabilities\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # 7. Apply Attention to Values\n",
        "        output = attention_weights @ v\n",
        "        assert output.shape == (B, self.num_heads, T, self.head_size)\n",
        "\n",
        "        # 8. Concatenate Multi-Head Results\n",
        "        output = output.transpose(1, 2).contiguous().view(B, T, self.embed_size)\n",
        "        assert output.shape == (B, T, self.embed_size)\n",
        "\n",
        "        # 9. Final Output Projection\n",
        "        output = self.c_proj(output)\n",
        "        assert x.shape == (B, T, self.embed_size)\n",
        "        assert output.shape == (B, T, self.embed_size)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c70a22-ba31-4aed-a948-85439dac2349",
      "metadata": {
        "id": "60c70a22-ba31-4aed-a948-85439dac2349"
      },
      "source": [
        "## TODO 5 - Summary\n",
        "This part is the Self-Attention Block of the network.\n",
        "\n",
        "The Self-Attention layer is the most important layer of the transformer and it builds the relations between the tokens in the sequence\n",
        "\n",
        "This layer is executed per token:\n",
        "* The input is a 3D of embedding vectors:\n",
        "  * There are B batches of sequences (1st dimension)\n",
        "  * There are T tokens per sequence (2nd dimension)\n",
        "  * Each token is represented with an embedding size of C (3rd dimension)\n",
        "* The first layer is a linear one that extends the embedding size of each token to 3-time of the original size, and actually make 3 instances of each token: query, key, value\n",
        "* Then, each one of the embeddings is splitted into multiple heads, which actually extends the 3D original input into 4D (embedding size -> number of heads * head size):\n",
        "  * It allows us to to attend to different types of information in parallel (As the number of heads).\n",
        "  * Each head learns a different representation/subspace of the data.\n",
        "  * It improves expressivity and generalization.\n",
        "* After that, we compute the score based on the dot product between the query matrix and the transpose of the keys matrix, allows us to learn the relation between tokens (As the score higher, the relation is stronger).\n",
        "* We also normalize the scores with the square of the head size in order to keep the scores within a reasonable range\n",
        "* Then we fill zero-mask on the scores for every \"future\" token. We want the scores will be just between each token and it's previous token, not the next tokens in order to avoid cheating (To really learn predicting without knowing what's going to be next).\n",
        "* Then we make a softmax activation of the score in order to make a probability distribution (weights) of each relation. Each weight represents the level of relation in the context between the current token and the previous one\n",
        "* Then we make a dot product between the weights and the values vectors. That's because during the softmax, we lost the embedding information about the token, and we got just scalar values about the probabilites. Now we have to connect the probabilities back to their token embeddings.\n",
        "* Then we turn back the 4D tensor to the original structure of 3D (number of heads * head size -> embedding size), since we've done with the multi-head attention and it's no longer needed\n",
        "* At the end we apply the second layer which is a linear one. This is a projection layer that mixes information between the heads and makes the training to not look at each head seperately.\n",
        "\n",
        "Now we have a output tensor with the same size of the original one, but with full multi-head attention between each token to all of the previous tokens in the sequence, with all the heads mixed, and each \"value\" token is attached to it's weight probability for every other token in the sequence.\n",
        "\n",
        "Since the guide was clear, it helped me a lot during implementation. However, I got some issues with the tensor shape of the \"scale\" tensor and also applying the casual masking correctly was challenging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62da9c70",
      "metadata": {
        "id": "62da9c70"
      },
      "source": [
        "## Transformer Block\n",
        "\n",
        "The transformer block is a single layer containing two main components: **Multi-Head Attention** followed by **MLP (Multi-Layer Perceptron)**. This layer design gets stacked multiple times to create the full transformer.\n",
        "\n",
        "### Layer Structure\n",
        "\n",
        "**Multi-Head Attention Layer**: Allows each position to gather information from other positions in the sequence. Multiple attention heads work in parallel, each potentially focusing on different types of relationships.\n",
        "\n",
        "**MLP Layer**: Processes each position independently through a feed-forward network, applying learned transformations to refine the representations.\n",
        "\n",
        "### Residual Connections - The Critical Bridge\n",
        "\n",
        "**What Residual Connections Are**: Instead of just passing the output forward, you add the original input back to the output: `output = input + layer(input)`. This creates a direct path for information to flow unchanged.\n",
        "\n",
        "**Why Residual Connections Are Essential**:\n",
        "- **Gradient Flow**: In deep networks, gradients can vanish as they backpropagate through many layers. Residuals provide direct gradient highways.\n",
        "- **Identity Preservation**: The network can learn to keep useful information unchanged by making the layer output close to zero.\n",
        "- **Easier Learning**: The layer only needs to learn the \"difference\" or \"refinement\" rather than completely new representations.\n",
        "- **Stable Training**: Without residuals, very deep transformers become difficult or impossible to train.\n",
        "\n",
        "### Layer-by-Layer Processing\n",
        "\n",
        "**First Layer - Multi-Head Attention**:\n",
        "1. Normalize the input\n",
        "2. Apply multi-head attention to capture inter-position relationships  \n",
        "3. Add the original input back (residual connection)\n",
        "\n",
        "**Second Layer - MLP**:\n",
        "1. Normalize the current representations\n",
        "2. Apply feed-forward transformations to each position\n",
        "3. Add the pre-MLP representations back (residual connection)\n",
        "\n",
        "The residual connections ensure that even in a 50-layer transformer, information and gradients can flow directly from the first layer to the last, enabling the training of very deep and powerful models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d00b5be5",
      "metadata": {
        "id": "d00b5be5"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Block, self).__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.embed_size)\n",
        "        self.ln2 = nn.LayerNorm(config.embed_size)\n",
        "        self.mlp = Mlp(config)\n",
        "        self.self_attn = SelfAttentionBlock(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x + self.self_attn(x))\n",
        "        x = self.ln2(x + self.mlp(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4dae08e-7e70-4b3d-b351-ebc334671aea",
      "metadata": {
        "id": "c4dae08e-7e70-4b3d-b351-ebc334671aea"
      },
      "source": [
        "## TODO 6 - Summary\n",
        "This part is the Transofmer Block of the network.\n",
        "\n",
        "This block combines both the Self Attention Block and the Feed Forward Block\n",
        "\n",
        "This block is executed per token:\n",
        "* The input is a 3D embedding vector where each token is represented with a size of *embed size* (There are T tokens per sequence and B sequences per batch)\n",
        "* The first layer is a norm layer that makes a normalization of the residual addition of the original tensor with the output of the self attention vector\n",
        "* The second layer, also a norm layer, makes a normalization of the residual addition of the output of the previous layer with the output of the MLP layer based on the output of the previous layer\n",
        "\n",
        "The residual addition allows to:\n",
        "* Help gradients flow backward without vanishing or exploding.\n",
        "* Allows the model to retain and blend the original input with the transformed features.\n",
        "\n",
        "The final output of the transofmer block contains:\n",
        "* The token itself\n",
        "* The relationship to all other tokens (via self-attention)\n",
        "* Includes nonlinear transformation via the MLP\n",
        "* Normalization for training stability\n",
        "\n",
        "The final output has rich and cotextual embedding information between the tokens in the sequence\n",
        "\n",
        "It was a quite straight-forward part with no any major issues"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1274aa",
      "metadata": {
        "id": "fb1274aa"
      },
      "source": [
        "## Transformer Model\n",
        "\n",
        "\n",
        "The `GPT2` class represents the complete transformer model - a decoder-only architecture that processes text and predicts the next token. This is the full model that combines all the components you've built.\n",
        "\n",
        "### Model Architecture Overview\n",
        "\n",
        "**Embedding Layers**: Convert discrete tokens into continuous vector representations that the neural network can process.\n",
        "\n",
        "**Transformer Stack**: Multiple transformer blocks stacked together, each refining the representations further.\n",
        "\n",
        "**Output Layer**: Converts the final representations back to vocabulary predictions for next-token generation.\n",
        "\n",
        "### Core Components\n",
        "\n",
        "**Token Embedding (`wte`)**: Maps each vocabulary word to a learned vector representation. Transforms token IDs into dense embeddings of size `embed_size`.\n",
        "\n",
        "**Position Embedding (`wpe`)**: Adds positional information since attention doesn't inherently understand word order. Each position gets its own learned embedding.\n",
        "\n",
        "**Transformer Blocks**: Stack of identical blocks (typically 12-48 layers) that progressively refine the representations through attention and feed-forward processing.\n",
        "\n",
        "**Final Layer Norm (`ln_f`)**: Normalizes the output from all transformer blocks before making predictions.\n",
        "\n",
        "**Language Model Head (`lm_head`)**: Linear layer that converts final embeddings back to vocabulary logits for next-token prediction.\n",
        "\n",
        "### Forward Pass Flow\n",
        "\n",
        "**Input Processing**:\n",
        "1. Convert token IDs to embeddings\n",
        "2. Add positional embeddings to provide sequence order information\n",
        "3. Combine token and position embeddings element-wise\n",
        "\n",
        "**Transformer Processing**:\n",
        "- Pass through each transformer block sequentially\n",
        "- Each block applies attention and MLP with residual connections\n",
        "- Representations become increasingly sophisticated through the stack\n",
        "\n",
        "**Output Generation**:\n",
        "1. Apply final normalization to stabilize outputs\n",
        "2. Project to vocabulary size using the language model head\n",
        "3. Compute loss against targets if provided for training\n",
        "\n",
        "### Key Design Decisions\n",
        "\n",
        "**Weight Sharing**: The token embedding and output projection share the same weights, reducing parameters and creating symmetry between input and output representations.\n",
        "\n",
        "**Autoregressive Training**: During training, the model predicts each position based only on previous positions, learning the sequential nature of language.\n",
        "\n",
        "**Next-Token Prediction**: The fundamental task is predicting the probability distribution over the vocabulary for what token should come next at each position.\n",
        "\n",
        "This architecture enables the model to generate coherent text by learning statistical patterns in language through the simple but powerful objective of next-token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e3ba1966",
      "metadata": {
        "id": "e3ba1966"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.embed_size), # token embeddings\n",
        "            wpe = nn.Embedding(config.block_size, config.embed_size), # positional embeddings\n",
        "            blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)]), # transformer blocks\n",
        "            ln_f = nn.LayerNorm(config.embed_size)\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size) # output layer for next token prediction\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        # sharing weights between token embedding and output layer\n",
        "        self.transformer.wte.weight = self.lm_head.weight # this allows the model to use the same embeddings for input and output tokens\n",
        "\n",
        "    def forward(self, idx, target=None):\n",
        "        \"\"\" Forward pass of the GPT-2 model.\n",
        "        Args:            idx: Input tensor of shape (B, T) where B is batch size and T is sequence length\n",
        "            target: Optional target tensor of shape (B, T) for computing loss\n",
        "        Returns:            logits: Output tensor of shape (B, T, vocab_size) containing logits for next token\n",
        "            loss: Optional scalar loss value if target is provided\n",
        "        \"\"\"\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.config.block_size, f\"Input tokens length {T} exceeds maximum length {self.config.block_size}\"\n",
        "\n",
        "        idx = idx.to(torch.long)\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=self.config.device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = tok_emb + pos_emb.unsqueeze(0)\n",
        "        for transformer_block in self.transformer.blocks:\n",
        "            x = transformer_block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if target is not None:\n",
        "            target = target.type(torch.LongTensor).to(self.config.device)\n",
        "            # compute the loss if we are given targets\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                target.view(-1),\n",
        "                ignore_index=-1,\n",
        "            )\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        assert logits.shape == (B, T, self.vocab_size)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b181041c-f0df-4310-9d78-e1b6097d7ec2",
      "metadata": {
        "id": "b181041c-f0df-4310-9d78-e1b6097d7ec2"
      },
      "source": [
        "## TODO 7 - Summary\n",
        "This part is the Final GPT2 Model, the \"customer facing\" model.\n",
        "\n",
        "This block combines all our previous work before.\n",
        "\n",
        "This block is executed per sequence of tokens:\n",
        "* The input is a 2D tensor with B sequences (B is the batch size), and each sequence contains T tokens (T is the sequence length). This input represent the input text that we want to predict the next token based on it.\n",
        "* During training only, there is an additional 2D tensor with same shape as the main input, and it contains the \"shift-right\" text of the main input. It contains the TRUE next token of the main input which we want to predict, and used to compute the loss and measure ourselves. Not relevant on inference.\n",
        "* The model contains a block size limit (The maximal number of tokens it can process in a sequence), so the input sequence lenght cannot exceed this limit.\n",
        "* The first layer is an embedding layer which converts each token in the sequence of the main input to an embedding vector -> A vector representation of each token which can contain many properties of it unlike a single integer of the original token. It converts the input tensor from 2D to 3D (Adds an embedding vector dimension).\n",
        "* The second layer, also an embedding layer, creates positional embeddings. We create a range tensor from 0 to T (The sequence length) [0,1,2,3,4..,T] and converts each position into an embedding. It allows us to know the relative position of each token to every other tokens.\n",
        "* After that we do a residual addition between the outputs of these two layers.\n",
        "* Then it's the major part of the model. We make some transfomer layers (As the number of layers we pass in configuration), and each time the input is processed using the Transformer block and it's output passed to the next Transformer block.\n",
        "  * We need multiple transofmer blocks because each layer denotes another perspective of the context. First layer looks for connection between relatively close tokens, and the last layers look at the whole text and the connection between each token to the entire text\n",
        "* After the transofmers we have a normalization layer to the output to avoid vanishing gradient\n",
        "* Then, the last layer converts the output of the previous layer (Embeedings of each token), to the final output which contains the Logits of each token. Now the logits vector of each token is as the same size of the vocabulary, and each item in the vector contains the logit of each token in the vocabulary. These logits are the unnormalized probabilities to gather each one of the tokens in the vocabulary after the token they represent.\n",
        "\n",
        "The final output is a 3D tensor with B sequences (B is the batch size), and each sequence contains T tokens (T is the sequence length), and each token contains a logits vector with the size of the vocabulary, where each token in a vocabulary gets a logit (a weight).\n",
        "\n",
        "* During training, after we compute the output logits, we also compute the loss function between the output logits (The predicted tokens) and the target tensor (The ground truth). We use a standard CrossEnthropy loss to know how we're accurate\n",
        "\n",
        "It was a challenging part:\n",
        "* I had many issues using shapes, types (Float / Long) and device matching.\n",
        "* Also realizing what exactly the purpose of the position, and when to use the target tensor was also challenging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beed0f77",
      "metadata": {
        "id": "beed0f77"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2030245e",
      "metadata": {
        "id": "2030245e"
      },
      "source": [
        "### Sampling Strategies\n",
        "\n",
        "How we convert these probabilities into actual text generation:\n",
        "\n",
        "#### 1. Greedy Decoding\n",
        "```python\n",
        "# Always pick the highest probability token\n",
        "def greedy_sample(logits):\n",
        "    return torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Result: Deterministic but potentially repetitive\n",
        "```\n",
        "\n",
        "#### 2. Temperature Sampling\n",
        "```python\n",
        "# Control randomness with temperature\n",
        "def temperature_sample(logits, temperature=0.8):\n",
        "    # Higher temperature = more random\n",
        "    # Lower temperature = more focused\n",
        "    scaled_logits = logits / temperature\n",
        "    probs = torch.softmax(scaled_logits, dim=-1)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "# Temperature effects:\n",
        "# T=0.1: \"To be or not to be, that is the question\"  (very predictable)\n",
        "# T=0.8: \"To be or not to be, that troubles the mind\"  (creative but coherent)\n",
        "# T=2.0: \"To be or not to fish, purple dreams loudly\"  (too random)\n",
        "```\n",
        "\n",
        "#### 3. Top-k Sampling\n",
        "```python\n",
        "# Only consider the k most likely tokens\n",
        "def top_k_sample(logits, k=50):\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    probs = torch.softmax(values, dim=-1)\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    return indices[next_token]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "078605dd",
      "metadata": {
        "id": "078605dd"
      },
      "source": [
        "Implement a top-k sampling  function to generate text from the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0954f313",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0954f313",
        "outputId": "7cb9d5c3-f0da-4fcb-d6e8-8e251513d9be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ngenerate_text = pipeline(model, tokenizer)\\nprompt = \"To be or not to be, that is the question: \"\\ngenerated_text = generate_text(prompt, max_len=100)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Only consider the k most likely tokens\n",
        "def top_k_sample(logits, k=50):\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    probs = torch.softmax(values, dim=-1).squeeze()\n",
        "    next_token = torch.multinomial(probs, num_samples=1)\n",
        "    indices = indices.squeeze()\n",
        "    return indices[next_token]\n",
        "\n",
        "def pipeline(model, tokenizer, k=10):\n",
        "    \"\"\"    Create a text generation pipeline for the given model and tokenizer.\n",
        "    Args:\n",
        "        model: The trained model instance\n",
        "        tokenizer: The tokenizer instance\n",
        "        k: Number of top-k tokens to sample from at each step (default: 10)\n",
        "    Returns:A function that takes a prompt and generates text\"\"\"\n",
        "    # step-by-step explanation of the pipeline function:\n",
        "    \"\"\"\n",
        "    1. Set the model to evaluation mode\n",
        "    2. Encode the input prompt using the tokenizer\n",
        "    3. Convert the encoded prompt to a tensor and move it to the model's device\n",
        "    4. While the length of the generated sequence is less than max_len:\n",
        "        a. Get the model's logits for the last token in the sequence\n",
        "        b. Extract the logits for the last token and apply softmax to get probabilities\n",
        "        c. Use top-k sampling to select the next token based on probabilities\n",
        "        d. Append the selected token to the sequence\n",
        "    5. Decode the generated sequence back to text using the tokenizer\n",
        "    6. Return the generated text\n",
        "    \"\"\"\n",
        "    def generate(prompt, max_len=1024):\n",
        "        #1. Set the model to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # 2. Encode the input prompt using the tokenizer\n",
        "        # 3. Convert the encoded prompt to a tensor and move it to the model's device\n",
        "        generated_tensor = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
        "        cur_len = generated_tensor.size(1)\n",
        "\n",
        "        # Add padding before the prompt to support the block size, but keep the cur_len as before padding\n",
        "        pad_len = model.config.block_size - generated_tensor.size(1)\n",
        "        if pad_len > 0:\n",
        "          padding = torch.zeros((1, pad_len), dtype=torch.long).to(device)\n",
        "          generated_tensor = torch.cat([padding, generated_tensor], dim=1)\n",
        "\n",
        "        # 4. While the length of the generated sequence is less than max_len:\n",
        "        while(cur_len < max_len):\n",
        "            # a. Get the model's logits for the last token in the sequence\n",
        "            # If the prompt is shorter than block size, pad on the left\n",
        "            logits, _ = model(generated_tensor[:, -model.config.block_size:])\n",
        "\n",
        "            # b. Extract the logits for the last token and apply softmax to get probabilities\n",
        "            # c. Use top-k sampling to select the next token based on probabilities\n",
        "            last_logit = logits[:, -1, :]\n",
        "            next_token = top_k_sample(last_logit, k).unsqueeze(0)\n",
        "\n",
        "            # d. Append the selected token to the sequence\n",
        "            generated_tensor = torch.cat([generated_tensor, next_token], dim=1)\n",
        "\n",
        "            cur_len += 1\n",
        "\n",
        "        # 5. Decode the generated sequence back to text using the tokenizer\n",
        "        # 6. Return the generated text\n",
        "        generated_tensor_np = generated_tensor.cpu().numpy().flatten()\n",
        "\n",
        "        # Find the last non-zero index\n",
        "        nonzero_indices = np.nonzero(generated_tensor_np)[0]\n",
        "        fisrt_nonzero_idx = nonzero_indices[0]\n",
        "        last_nonzero_idx = nonzero_indices[-1]\n",
        "        generated_tensor_np = generated_tensor_np[fisrt_nonzero_idx:last_nonzero_idx+1]\n",
        "\n",
        "        return tokenizer.decode(generated_tensor_np)\n",
        "    return generate\n",
        "\n",
        "\n",
        "# exmple usage of the pipeline\n",
        "\"\"\"\n",
        "generate_text = pipeline(model, tokenizer)\n",
        "prompt = \"To be or not to be, that is the question: \"\n",
        "generated_text = generate_text(prompt, max_len=100)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aa9aa90-1a5d-4b61-b708-d439d0933811",
      "metadata": {
        "id": "3aa9aa90-1a5d-4b61-b708-d439d0933811"
      },
      "source": [
        "## TODO 8 - Summary\n",
        "\n",
        "Although it was just not a part of the model architecture, it was one of the most challenging parts in the assignment, probably because it was less supervised part than other parts in the assignments.\n",
        "\n",
        "In this part, the mission is to generate text based on a prompt, exactly like ChatGPT (The inference part)\n",
        "\n",
        "* The input is a textual prompt, and the max lenght of the text we're willing to generate\n",
        "* Then, we first run the model on evaluation mode.\n",
        "* Then we create our generated tensor that contains the prompt itself, encoded to integers using the Tokenizer I implemented previously.\n",
        "* In addition, since the model didn't work well when the sequence length was small (below the block size), I added left padding to the prompt in order to pass an input with an exact size of the model block size. Then I created a zero tensor with the size of the block size minus the prompt lenght, and padded it to the left of the prompt tensor\n",
        "  * I kept the \"cur_len\" index as same as the prompt len, since we don't want it to count also the zero paddings in the beginning.\n",
        "* Now I got into a loop, while we don't reach the max lenght we're willing to generate (Each iteration adds one more token to the text).\n",
        "* In each iteration, I called the GPT Model using the current generated tensor, in order to get the next token (Since the model cannot get a sequence that exceeds the block size of the model, I passed the last \"block size\" tokens of the current generated tensor)\n",
        "* Since it's inference mode, I passed just the current generated tensor as input, with no target\n",
        "* I got as output a tensor of logits for each token in the sequence, however we're interested just in the logits of the last token in the sequence (Just for the last token we predict a next token)\n",
        "* Then I implemented a top-k function to predict the next token based on the logits:\n",
        "  * First we get the top k values and indices (encoded tokens) from the logits\n",
        "  * Then we convert them to normalized probabilites using softmax\n",
        "  * Then we extract the token with the highest probability\n",
        "* After we have the next token (as it's integer representation), we concatinate it to the current generated tensor\n",
        "* After we finished generating, we convert the tensor to a simple 1D numpy array we can use\n",
        "* Before decoding the numpy array back to characters, we first remove the trailing zero paddings, both in the start and in the end of the array\n",
        "* Now we decode using the Tokenizer we built in the first part the integers back to characters and return the generated text\n",
        "\n",
        "It was a really challenging part, including:\n",
        "* Many shape and dimension related issues - It took a long time of troubleshooting to make a complete matching with dimensions and shape between tensors.\n",
        "* At first, also calling the model raised errors from the self attention layer, since the masking was on fixed size and not a dynamic size ```scores.masked_fill(self.mask == 0, float('-inf')) -> scores.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))```\n",
        "* After a final success run of the generation, I got a total geebrish in the first ~260 characters, which is the size of the block size\n",
        "  * After a long investigation, I solved it with adding left padding to the tensor. I found the predictions are not working well when the sequence lenght is much less than the block size\n",
        "  * Since I added the zero-padding, I also needed to remove them at the finish of the process before decoding back to characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37dff197",
      "metadata": {
        "id": "37dff197"
      },
      "source": [
        "### Conig the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ee3dd390",
      "metadata": {
        "id": "ee3dd390"
      },
      "outputs": [],
      "source": [
        "# This is the most minmal config class for GPT-2 (Takes 40-60 to train). You are welcome to modify it to a bigger model or for loger but no the other way around (expet of batch size which you can reduce if you run out of memory)\n",
        "class GPT2Config:\n",
        "    def __init__(self):\n",
        "        self.embed_size = 64\n",
        "        self.num_heads = 2\n",
        "        self.num_layers = 4\n",
        "        self.vocab_size = len(tokenizer)\n",
        "        self.block_size = 256\n",
        "        self.lr = 3e-4\n",
        "        self.batch_size = 32 # use a smaller batch size if you run out of memory\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.path = 'models/model.pth'\n",
        "        self.num_epochs = 10\n",
        "\n",
        "        self.patience = 5  # Early stopping patience\n",
        "        self.grad_clip = 1.0  # Gradient clipping threshold\n",
        "\n",
        "        # Learning rate scheduler configs\n",
        "        self.use_scheduler = True\n",
        "        self.scheduler_type = 'reduce_on_plateau'  # 'reduce_on_plateau', 'cosine', 'step'\n",
        "        self.lr_patience = 2  # For ReduceLROnPlateau\n",
        "        self.lr_factor = 0.5  # For ReduceLROnPlateau\n",
        "        self.step_size = 1  # For StepLR\n",
        "        self.gamma = 0.1  # For StepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3202876e",
      "metadata": {
        "id": "3202876e"
      },
      "source": [
        "## Traninig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f822ddc5",
      "metadata": {
        "id": "f822ddc5"
      },
      "outputs": [],
      "source": [
        "# traning function\n",
        "def train_model(model, train_loader, val_loader, optimizer, config=None):\n",
        "\n",
        "    # Setup\n",
        "    model.train()\n",
        "    summary_writer = SummaryWriter(log_dir='runs/shakespeare_experiment')\n",
        "\n",
        "    # Create models directory\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "\n",
        "    # Training state tracking\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    patience = getattr(config, 'patience', 5)  # Early stopping patience\n",
        "\n",
        "    # Loss tracking\n",
        "    train_losses = deque(maxlen=1000)\n",
        "    global_step = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    print(f\"Training for {config.num_epochs} epochs\")\n",
        "    print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\") as pbar:\n",
        "            for batch_idx, (x, y) in enumerate(pbar):\n",
        "                x, y = x.to(config.device), y.to(config.device)\n",
        "\n",
        "                # Forward pass\n",
        "                optimizer.zero_grad()\n",
        "                logits, loss = model(x, y)\n",
        "\n",
        "                # Backward pass\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping (optional but recommended)\n",
        "                if hasattr(config, 'grad_clip') and config.grad_clip > 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track losses\n",
        "                loss_item = loss.item()\n",
        "                train_losses.append(loss_item)\n",
        "                epoch_train_losses.append(loss_item)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'loss': f\"{loss_item:.4f}\",\n",
        "                    'avg_loss': f\"{np.mean(train_losses):.4f}\",\n",
        "                    'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
        "                })\n",
        "\n",
        "                # Log to tensorboard every N steps\n",
        "                if global_step % 10 == 0:  # Log every 10 steps\n",
        "                    summary_writer.add_scalar('Loss/train_step', loss_item, global_step)\n",
        "                    summary_writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], global_step)\n",
        "\n",
        "                global_step += 1\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\", leave=False) as val_pbar:\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(config.device), y.to(config.device)\n",
        "                    logits, loss = model(x, y)\n",
        "                    val_losses.append(loss.item())\n",
        "                    val_pbar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        avg_train_loss = np.mean(epoch_train_losses)\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "\n",
        "        # Log epoch metrics\n",
        "        summary_writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)\n",
        "        summary_writer.add_scalar('Loss/validation_epoch', avg_val_loss, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "        # Model checkpointing\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'global_step': global_step\n",
        "        }\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(checkpoint, os.path.join('models', 'best_model.pt'))\n",
        "            print(f\"New best model saved! Val loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Save regular checkpoint\n",
        "        torch.save(checkpoint, os.path.join('models', f'checkpoint_epoch_{epoch}.pt'))\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
        "            break\n",
        "\n",
        "        # Save latest model using your custom save method\n",
        "        if hasattr(model, 'save_model'):\n",
        "            model.save_model(config.path)\n",
        "\n",
        "    # Final logging\n",
        "    summary_writer.close()\n",
        "    print(\"Training complete!\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'final_train_loss': avg_train_loss,\n",
        "        'final_val_loss': avg_val_loss,\n",
        "        'epochs_trained': epoch + 1\n",
        "    }\n",
        "\n",
        "def estimate_loss(model, eval_iters, data_loaders={}):\n",
        "    \"\"\"        Estimate the loss of the model on the given splits.\n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        eval_iters: Number of iterations to average the loss over\n",
        "        splits: List of dataset splits to evaluate (e.g., 'train', 'validation')\n",
        "        data_loader: DataLoader instance for fetching batches\n",
        "    Returns:\n",
        "        out: Dictionary with average loss for each split\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        out = {}\n",
        "        model.eval()\n",
        "        for split, dl in data_loaders.items():\n",
        "            losses = 0\n",
        "            for i in range(eval_iters):\n",
        "                x, y = next(iter(dl))\n",
        "                x, y = x.to(model.config.device), y.to(model.config.device)\n",
        "                _, loss = model(x, y)\n",
        "                losses += loss.item()\n",
        "            out[split] = losses / eval_iters\n",
        "        model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a5bfaee8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5bfaee8",
        "outputId": "b91492f3-9acf-4699-a23c-d4f4537ea1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model\n",
            "device: cuda\n",
            "Total parameters: 220,673\n"
          ]
        }
      ],
      "source": [
        "# init config and datasets\n",
        "con = GPT2Config()\n",
        "train_dataset = ShakespeareDataset(train_data, con.block_size)\n",
        "val_dataset = ShakespeareDataset(val_data, con.block_size)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=con.batch_size,\n",
        "    shuffle=shuffle_train,\n",
        "    num_workers=4,\n",
        "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=con.batch_size,\n",
        "    shuffle=False,  # Don't shuffle validation data\n",
        "    num_workers=4,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "model = GPT2(con)\n",
        "\n",
        "if LOAD_PRETRAINED_MODEL:\n",
        "  state_dict = torch.load(os.path.join('models', 'best_model.pt'), map_location=con.device, weights_only=False)\n",
        "  model.load_state_dict(state_dict['model_state_dict'])\n",
        "  print(\"Loaded pretrained model\")\n",
        "\n",
        "model.to(con.device)\n",
        "print(f\"device: {con.device}\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=con.lr)\n",
        "\n",
        "total_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    total_params += param.numel()\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7dd3da7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7dd3da7",
        "outputId": "ec79603a-1ccd-4747-f8a6-d335a5a9a88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Training for 10 epochs\n",
            "Training batches: 31362, Validation batches: 3478\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "Training Epoch 1:   0%|          | 0/31362 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.53it/s, loss=0.0070, avg_loss=0.0086, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train Loss: 0.6579, Val Loss: 0.0087\n",
            "New best model saved! Val loss: 0.0087\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:47<00:00, 48.44it/s, loss=0.0062, avg_loss=0.0072, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train Loss: 0.0078, Val Loss: 0.0077\n",
            "New best model saved! Val loss: 0.0077\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:47<00:00, 48.40it/s, loss=0.0089, avg_loss=0.0066, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train Loss: 0.0068, Val Loss: 0.0073\n",
            "New best model saved! Val loss: 0.0073\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.51it/s, loss=0.0076, avg_loss=0.0064, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train Loss: 0.0065, Val Loss: 0.0070\n",
            "New best model saved! Val loss: 0.0070\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.54it/s, loss=0.0049, avg_loss=0.0061, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train Loss: 0.0062, Val Loss: 0.0069\n",
            "New best model saved! Val loss: 0.0069\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.51it/s, loss=0.0058, avg_loss=0.0060, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Train Loss: 0.0061, Val Loss: 0.0068\n",
            "New best model saved! Val loss: 0.0068\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.51it/s, loss=0.0068, avg_loss=0.0059, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Train Loss: 0.0059, Val Loss: 0.0067\n",
            "New best model saved! Val loss: 0.0067\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:46<00:00, 48.54it/s, loss=0.0047, avg_loss=0.0058, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Train Loss: 0.0059, Val Loss: 0.0066\n",
            "New best model saved! Val loss: 0.0066\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:48<00:00, 48.35it/s, loss=0.0057, avg_loss=0.0058, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 - Train Loss: 0.0058, Val Loss: 0.0065\n",
            "New best model saved! Val loss: 0.0065\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31362/31362 [10:48<00:00, 48.35it/s, loss=0.0059, avg_loss=0.0058, lr=3.00e-04]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 - Train Loss: 0.0057, Val Loss: 0.0065\n",
            "New best model saved! Val loss: 0.0065\n",
            "Training complete!\n",
            "Best validation loss: 0.0065\n"
          ]
        }
      ],
      "source": [
        "if not LOAD_PRETRAINED_MODEL:\n",
        "  # Creating an optimizer\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=con.lr, weight_decay=0.01)\n",
        "\n",
        "  # train the model\n",
        "  results = train_model(model, train_loader, val_loader, optimizer, con)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "89517e6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89517e6e",
        "outputId": "f626fd84-e17a-46c4-9c4f-ed42d9214aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to be or not to be!\n",
            "\n",
            "'O:', wae?\n",
            "\n",
            "I'tro;\n",
            "I no? I gente!\n",
            "I fot, woo!-\n",
            "\n",
            "Fie:\n",
            "I h! ay, I hate!\n",
            "\n",
            "o'o?\n",
            "\n",
            "o ten, to homa go:'te be!\n",
            "\n",
            "CAPULed so sun; hours; farewell:\n",
            "'Tittain have the hatef, his fairly is my lutt with shalt wand yince, mince of sweet, where is the ten\n",
            "Things hange her those my sworn, where,' sortance\n",
            "That I am nor warm my broken he husbant,\n",
            "But thus he compuns. He hath denied than arm\n",
            "Thlowing take sham battmen. Angelo, and ye supply.\n",
            "\n",
            "SICINIUS:\n",
            "I cannot ward I thou sorrow,\n",
            "\n",
            "FRIAR, Chence to your marks a dir to be whose behite stood.\n",
            "This he is spoke all no does how I will wholers.\n",
            "\n",
            "BENVOLIO$:\n",
            "Hhy nature to put thou will you, thou not\n",
            "In my treadut so, have may to his son?\n",
            "\n",
            "LUCENTIO:\n",
            "Not our hanging have him that I am no to cenquire.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Beseech your makes with the dage all blow;\n",
            "Which is slaughters of the stops,\n",
            "His lives and thou shall have is sakes in my blocks? were wood for whose of love with,\n",
            "To despated, whose coursels by patience\n",
            "Be that in him.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Ay, let yours?\n",
            "\n",
            "COPU\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline(model, tokenizer, k=10)\n",
        "print(generator('to be or not to be', max_len=1024))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}