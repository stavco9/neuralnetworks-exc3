{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9cc8f0",
   "metadata": {},
   "source": [
    "# Assingment 3: Shaikspear Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ef68b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Your task is to build a transformer model from scratch and train it on Shakespeare's works. By the end of this assignment, you'll implement the core concepts behind transformers - the architecture that powers modern language models like GPT, BERT, and many others.\n",
    "\n",
    "### Overview of Transformer Architecture\n",
    "\n",
    "Transformers revolutionized natural language processing by introducing the self-attention mechanism, which allows the model to process all positions in a sequence simultaneously, rather than sequentially like RNNs. The key innovations include:\n",
    "\n",
    "1. **Self-Attention**: Allows each position to attend to all positions in the previous layer\n",
    "2. **Multi-Head Attention**: Runs multiple attention operations in parallel\n",
    "3. **Positional Encoding**: Adds information about the position of tokens in the sequence\n",
    "4. **Feed-Forward Networks**: Processes the attended features\n",
    "5. **Layer Normalization**: Stabilizes training of deep networks\n",
    "6. **Residual Connections**: Helps with gradient flow in deep networks\n",
    "\n",
    "### What We're Building\n",
    "\n",
    "We'll implement a simplified GPT2-style (decoder-only) transformer that can:\n",
    "- Take a sequence of Shakespeare text as input\n",
    "- Predict the next character or word in the sequence\n",
    "- Generate new text in Shakespeare's style\n",
    "\n",
    "Let's begin by setting up our environment and loading the Shakespeare dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f4bcb",
   "metadata": {},
   "source": [
    "## Your Tasks\n",
    "\n",
    "Complete all the **TODO** sections in the provided code to build a working transformer model that can generate Shakespeare-style text.\n",
    "\n",
    "### Step-by-Step Instructions\n",
    "\n",
    "#### 1. **Read and Understand First** \n",
    "- **Before coding anything**, carefully read through the entire notebook\n",
    "- Pay special attention to the explanatory sections that describe:\n",
    "  - How transformers work\n",
    "  - The difference between encoder and decoder architectures\n",
    "  - The role of attention mechanisms\n",
    "  - Why we use masking in decoder blocks\n",
    "\n",
    "#### 2. **Complete the TODO Tasks in Order**\n",
    "\n",
    "**TODO 1: Character Tokenizer** (`CharacterTokenizer` class)\n",
    "- Implement character-to-index and index-to-character mappings\n",
    "- Create `encode()` and `decode()` methods\n",
    "- Test your tokenizer works correctly\n",
    "\n",
    "**TODO 2: Dataset Preparation** \n",
    "- Convert text to tensor format\n",
    "- Understand the input-target relationship for next-token prediction\n",
    "\n",
    "**TODO 3: Dataset Class** (`ShakespeareDataset`)\n",
    "- Implement `__getitem__()` method to generate training sequences\n",
    "- Ensure proper input-target shifting (input: \"To be or\", target: \"o be or \")\n",
    "\n",
    "**TODO 4: Multi-Layer Perceptron** (`Mlp` class)\n",
    "- Build a simple feed-forward network\n",
    "- Use GELU activation and proper dimensions (embed_size â†’ 4*embed_size â†’ embed_size)\n",
    "\n",
    "**TODO 5: Self-Attention Block** (`SelfAttentionBlock`)\n",
    "- **This is the most complex part** - implement the complete attention mechanism:\n",
    "  - Generate Query, Key, Value matrices\n",
    "  - Reshape for multi-head processing\n",
    "  - Compute attention scores and apply causal masking\n",
    "  - Apply softmax and combine with values\n",
    "  - Concatenate multi-head results and project output\n",
    "- Follow the detailed step-by-step guide provided in the comments\n",
    "\n",
    "**TODO 6: Transformer Block** (`Block` class)\n",
    "- Combine attention and MLP with residual connections\n",
    "- Apply layer normalization correctly\n",
    "\n",
    "**TODO 7: Full GPT-2 Model** (`GPT2` class)\n",
    "- Implement the complete forward pass\n",
    "- Combine token embeddings, positional embeddings, transformer blocks, and output projection\n",
    "- Handle loss computation for training\n",
    "\n",
    "**TODO 8: Text Generation Pipeline** (`pipeline` function)\n",
    "- Implement top-k sampling for text generation\n",
    "- Handle autoregressive generation (one token at a time)\n",
    "\n",
    "# 3. **Explain your code**\n",
    "- After completing each section, write in a markdown section with an explanation of how your code works and why you made certain design choices\n",
    "\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "You'll know you're successful when:\n",
    "- [ ] All TODO sections are completed without errors\n",
    "- [ ] The model trains successfully (loss decreases over time)\n",
    "- [ ] Generated text resembles Shakespeare's style (Dont expect perfection, It should be english text with some Shakespearean flair, probably with some nonsense or non coherent phrases)\n",
    "- [ ] You can explain how each component works\n",
    "\n",
    "### ðŸ’¡ Learning Goals\n",
    "\n",
    "By the end of this assignment, you should understand:\n",
    "- How transformers process sequential data\n",
    "- The role of attention in capturing relationships between tokens\n",
    "- Why masking is crucial for autoregressive models\n",
    "- How modern language models generate text\n",
    "- The architecture behind models like GPT, Claude, and ChatGPT\n",
    "\n",
    "Remember: This assignment is about understanding, not just completing code. Take your time to read and comprehend each section before implementing the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1d0dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2733494c4f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import kagglehub\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607a766",
   "metadata": {},
   "source": [
    "## Load  shakespeare.text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3794e0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "Number of unique characters: 65\n",
      "First 100 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "f_name = 'shakespeare.txt' # if the file in different location, change this line\n",
    "with open('shakespeare.txt', \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Length of text: {len(text)} characters\")\n",
    "print(f\"Number of unique characters: {len(set(text))}\")\n",
    "print(f\"First 100 characters:\\n{text[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a73ad71",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84578904",
   "metadata": {},
   "source": [
    "# Dataset Encoding and Tokenization\n",
    "\n",
    "## Understanding Text Encoding for Transformers\n",
    "\n",
    "Before we can feed text into our transformer model, we need to convert it into numerical representations. This process involves:\n",
    "1. **Tokenization**: Breaking text into smaller units (characters, subwords, or words)\n",
    "2. **Vocabulary Building**: Creating a mapping between tokens and unique integers\n",
    "3. **Encoding**: Converting text sequences into sequences of integers\n",
    "4. **Decoding**: Converting integer sequences back to readable text\n",
    "\n",
    "### Character-Level vs Word-Level Tokenization\n",
    "\n",
    "For this assignment, we'll use **character-level tokenization** \n",
    "\n",
    "\n",
    "Let's create a simple character-level tokenizer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7918b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    def __init__(self, text):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer by building vocabulary from input text.\n",
    "        \n",
    "        Args:\n",
    "            text: String containing all training text\n",
    "        \"\"\"\n",
    "        # Get all unique characters and sort them\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "\n",
    "        ## TODO: Create a mapping from characters to indices and vice versa via a simple dictionary \n",
    "\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert string to list of integers\"\"\"\n",
    "        ## TODO: Apply the mapping from characters to indices\n",
    "        pass\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        \"\"\"Convert list of integers back to string\"\"\"\n",
    "        ## TODO: Apply the mapping from indices to characters\n",
    "        pass   \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the vocabulary\"\"\"\n",
    "        return self.vocab_size\n",
    "\n",
    "# Assuming your text is loaded in a variable called 'text'\n",
    "# Create the tokenizer\n",
    "tokenizer = CharacterTokenizer(text)\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"To be or not to be\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Tokenizer vocab_size:\", len(tokenizer))\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cc32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of character frequencies\n",
    "char_counts = Counter(text)\n",
    "def plot_char_distribution(char_counts):\n",
    "    \"\"\"Plot the distribution of character frequencies.\"\"\"\n",
    "    chars, counts = zip(*char_counts.most_common(30))  # Get top 30 characters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(chars, counts)\n",
    "    plt.xlabel('Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Character Frequency Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "plot_char_distribution(char_counts)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ab122",
   "metadata": {},
   "source": [
    "Now let's prepare our dataset for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO: Create a torch tensor from the encoded text\n",
    "# data = ....\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "\n",
    "# Split into train and validation sets (90/10 split)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Training set size: {len(train_data):,} characters\")\n",
    "print(f\"Validation set size: {len(val_data):,} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186d1029",
   "metadata": {},
   "source": [
    "# Understanding Sequence Generation in Transformers\n",
    "\n",
    "## What is a Transformer Trained to Do?\n",
    "\n",
    "At its core, a transformer language model is trained to perform **next-token prediction**. This simple objective - predicting what comes next - enables the model to generate coherent text, understand language patterns, and even appear to \"understand\" context.\n",
    "\n",
    "### The Autoregressive Nature of Language Modeling\n",
    "\n",
    "Transformers for text generation are **autoregressive**, meaning they generate text one token at a time, using previously generated tokens as context for predicting the next one.\n",
    "\n",
    "```\n",
    "Given: \"To be or not to\"\n",
    "Predict: \"be\" (next token)\n",
    "\n",
    "Given: \"To be or not to be\"  \n",
    "Predict: \",\" (next token)\n",
    "\n",
    "Given: \"To be or not to be,\"\n",
    "Predict: \"that\" (next token)\n",
    "```\n",
    "\n",
    "### Training vs Generation: Two Different Processes\n",
    "\n",
    "#### During Training (Assume not a character-level tokenizer for simplicity....)\n",
    "```python\n",
    "# Training sees the full correct sequence\n",
    "Input:  [\"To\", \"be\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\"]\n",
    "Target: [\"be\", \"or\", \"not\", \"to\", \"be\", \",\", \"that\", \"is\"]\n",
    "\n",
    "# The model learns in parallel:\n",
    "Position 0: Given \"To\" â†’ predict \"be\"\n",
    "Position 1: Given \"To be\" â†’ predict \"or\"\n",
    "Position 2: Given \"To be or\" â†’ predict \"not\"\n",
    "... (all positions simultaneously)\n",
    "```\n",
    "\n",
    "#### During Generation (Autoregressive)\n",
    "```python\n",
    "# Generation builds up token by token\n",
    "Step 1: Input: \"To\" â†’ Model predicts: \"be\"\n",
    "Step 2: Input: \"To be\" â†’ Model predicts: \"or\"  \n",
    "Step 3: Input: \"To be or\" â†’ Model predicts: \"not\"\n",
    "... (sequential generation)\n",
    "```\n",
    "\n",
    "### The Probability Distribution\n",
    "\n",
    "At each step, the transformer doesn't just predict one token - it outputs a **probability distribution** over the entire vocabulary:\n",
    "\n",
    "```python\n",
    "# Example output probabilities after \"To be or not to\"\n",
    "{\n",
    "    \"be\": 0.7,      # High probability - this phrase often repeats\n",
    "    \"die\": 0.1,     # Possible alternative\n",
    "    \"sleep\": 0.05,  # Another thematic alternative\n",
    "    \"live\": 0.03,   # Less likely but possible\n",
    "    \",\": 0.02,      # Could end the phrase here\n",
    "    ... (probabilities for all tokens in vocabulary)\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Context Window and Attention\n",
    "\n",
    "The transformer's **context window** (block_size) determines how much previous text it can \"see\" when making predictions:\n",
    "\n",
    "### What the Model Actually Learns\n",
    "\n",
    "Through next-token prediction, the transformer learns:\n",
    "\n",
    "1. **Syntax and Grammar**\n",
    "   - \"To be or\" â†’ likely followed by \"not\"\n",
    "   - Verb conjugations, sentence structure\n",
    "\n",
    "2. **Semantic Relationships**\n",
    "   - \"king\" often appears near \"queen\", \"throne\", \"crown\"\n",
    "   - Thematic consistency within passages\n",
    "\n",
    "3. **Style and Register**\n",
    "   - Shakespearean vocabulary and phrasing\n",
    "   - Iambic pentameter patterns\n",
    "   - Archaic grammatical structures\n",
    "\n",
    "4. **Long-range Dependencies**\n",
    "   - Rhyme schemes across lines\n",
    "   - Character names and relationships\n",
    "   - Plot consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8054fc",
   "metadata": {},
   "source": [
    "In this assinigment, we train the model to predict the next character given the previous characters:\n",
    "- **Input**: \"To be or\"\n",
    "- **Target**: \"o be or \"\n",
    "In the above exmpale there are 8 trainig examples:\n",
    "1. 'T' -> 'o'\n",
    "2. 'To' -> ' '\n",
    "3. 'To ' -> 'b'\n",
    "4. 'To b' -> 'e'\n",
    "5. 'To be' -> ' '\n",
    "6. 'To be o' -> 'r'\n",
    "8. 'To be or' -> ' '\n",
    "\n",
    "Pay attention that the transformer will process all of them simultaneously, allowing it to learn the relationships between characters and their positions in the sequence. Also notice how the target is shifted by one position. This teaches the model to predict what comes next.\n",
    "\n",
    "Lets create a torch Dataset that generates these input-target pairs from our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c85788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Shakespeare text data.\n",
    "    Generates overlapping sequences for language modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, block_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Tensor of encoded text tokens\n",
    "            block_size: Length of each sequence (context length)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Calculate number of possible sequences\n",
    "        self.num_sequences = len(data) - block_size\n",
    "        \n",
    "        if self.num_sequences <= 0:\n",
    "            raise ValueError(f\"Data length ({len(data)}) must be greater than block_size ({block_size})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of possible sequences\"\"\"\n",
    "        return self.num_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sequence and its target.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sequence start position\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (input_sequence, target_sequence)\n",
    "        \"\"\"\n",
    "        ## TODO: implement the batch generation logic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33da7f",
   "metadata": {},
   "source": [
    "### Example Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f195c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "shuffle_train = True\n",
    "block_size = 128\n",
    "batch_size = 16\n",
    "train_dataset = ShakespeareDataset(train_data, block_size)\n",
    "val_dataset = ShakespeareDataset(val_data, block_size)\n",
    "\n",
    "print(f\"Training sequences available: {len(train_dataset):,}\")\n",
    "print(f\"Validation sequences available: {len(val_dataset):,}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_train,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Example of a training batch instance:\", end='\\n\\n')\n",
    "print(f\"Input:  '{tokenizer.decode(x[0].tolist())}'\") # Decode the first input sequence\n",
    "print(\"=\" * 50)\n",
    "print(f\"Target: '{tokenizer.decode(y[0].tolist())}'\") # Decode the first target sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35756c",
   "metadata": {},
   "source": [
    "## Transformer Architecture Overview\n",
    "The transformer is a neural network architecture that revolutionized how AI systems process language. Think of it as a sophisticated pattern-matching system that can understand relationships between words in a sentence, regardless of how far apart they are.\n",
    "\n",
    "**The Core Innovation: Attention**\n",
    "\n",
    "The transformer's key breakthrough is something called \"attention.\" Instead of reading text word-by-word like older systems, transformers look at all words simultaneously and figure out which ones are most relevant to each other. It's like having a conversation where you can instantly consider every word someone has said, not just the most recent ones.\n",
    "\n",
    "**Types of Attention Mechanisms**\n",
    "\n",
    "There are three main types of attention used in transformers:\n",
    "\n",
    "- **Self-Attention(Bidirectional)**: Each position in a sequence attends to all positions in the same sequence. This is like each word asking \"which other words in this sentence are most relevant to understanding me?\"\n",
    "\n",
    "- **Cross-Attention**: Positions in one sequence (like a decoder) attend to positions in a different sequence (like an encoder). This is used in encoder-decoder models where the decoder needs to \"look at\" the encoder's representations while generating output.\n",
    "\n",
    "- **Masked Self-Attention**: A restricted form of self-attention where each position can only attend to previous positions, not future ones. This maintains the causal structure needed for text generation.\n",
    "\n",
    "**How Masking Controls Encoder vs Decoder Behavior**\n",
    "\n",
    "The key difference between encoder and decoder blocks lies in their attention masking:\n",
    "\n",
    "- **Encoder blocks** use **bidirectional self-attention** with no masking. Each token can attend to all other tokens in the sequence, allowing the model to build rich contextual representations by considering the full context.\n",
    "\n",
    "- **Decoder blocks** use **causal masking** (also called autoregressive masking). Each token can only attend to itself and previous tokens, never future ones. This prevents the model from \"cheating\" during training by looking ahead, forcing it to learn to predict the next token based only on what came before.\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "The original transformer has two main parts:\n",
    "- **Encoder**: Takes input text and creates rich representations that capture meaning and context using bidirectional attention\n",
    "- **Decoder**: Uses those representations and generates output text using causal attention plus cross-attention to the encoder\n",
    "\n",
    "Both parts use \"self-attention\" layers that let each word position \"attend to\" or focus on other positions. This creates a web of connections showing which words are most important for understanding each other.\n",
    "\n",
    "**Modern Simplification: Decoder-Only Architecture**\n",
    "\n",
    "However, for many real-world applications, researchers discovered that using only the decoder portion is often sufficient and more efficient. This **decoder-only** approach is what powers popular language models like GPT, Claude, and most modern conversational AI systems. Instead of having separate encoder and decoder stacks, these models use multiple decoder blocks that generate text autoregressively - predicting one token at a time based on all previous tokens using causal masking. This is the architecture we'll be building in this notebook.\n",
    "\n",
    "**Key Components**\n",
    "\n",
    "- **Multi-head attention**: Instead of just one attention mechanism, it runs several in parallel, each potentially focusing on different types of relationships\n",
    "- **Feed-forward networks**: Simple neural networks that process the attention outputs\n",
    "- **Layer normalization and residual connections**: Technical elements that help training stability\n",
    "- **Positional encoding**: Since attention doesn't inherently understand word order, this adds position information\n",
    "\n",
    "**Why It Matters**\n",
    "\n",
    "Transformers can process text in parallel rather than sequentially, making them much faster to train. They're also better at capturing long-range dependencies - understanding how a word at the beginning of a paragraph relates to one at the end. The decoder-only variant proves particularly effective for text generation tasks, as it learns to model the probability distribution of natural language through next-token prediction. The masking mechanism is crucial here - it ensures the model learns proper causal relationships and can generate coherent text without access to future information. This architecture powers modern language models like GPT, BERT, and others that have transformed natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cef6f9",
   "metadata": {},
   "source": [
    "<img src=\"https://heidloff.net/assets/img/2023/02/transformers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a1253",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c96452a0",
   "metadata": {},
   "source": [
    "### Feed-Forward Network (MLP)\n",
    "Lets start from from simple to complicated. As the diagram shows, the MLP is a simple feed-forward neural network that processes the output of the attention mechanism. In the GPT2 architecture that we are following here it consists of two linear transformations with a GeLU activation in between. The MLp layer streach the dimention of the embedding from `embed_size` to `4*embed_size` and then back to `embed_size`.\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9efa430",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2be3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    ## TODO: Implement a simple MLP with two linear layers and GELU activation\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.embed_size = config.embed_size\n",
    "        ## TODO: Define the MLP architecture\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TODO: Implement the forward pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e9d3a1",
   "metadata": {},
   "source": [
    "## Multi Head Masked Self-Attention Implementation\n",
    "\n",
    "The self-attention mechanism is the core of the transformer architecture, allowing the model to weigh the importance of different words in a sequence relative to each other. This guide walks you through completing the provided implementation.\n",
    "\n",
    "### Understanding the Provided Setup\n",
    "\n",
    "You're given a `SelfAttentionBlock` class with the initialization already complete. Here's what's provided:\n",
    "\n",
    "**Linear Layers**:\n",
    "- `self.c_atten`: Creates Q, K, V matrices simultaneously (outputs `3 * head_size * num_heads`)\n",
    "- `self.c_proj`: Final output projection back to embedding size\n",
    "\n",
    "**Causal Mask**: \n",
    "- `self.bias`: Pre-computed lower triangular mask of shape `(1, 1, block_size, block_size)`\n",
    "- Prevents attending to future positions (essential for decoder behavior)\n",
    "\n",
    "**Key Constraint**: `head_size * num_heads = embed_size` (this is verified in the assertion)\n",
    "\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "\n",
    "1. **Input Processing**: Your input tensor `x` has shape `(B, T, C)` where:\n",
    "   - `B` = batch size (number of sequences processed together)\n",
    "   - `T` = sequence length (number of tokens in each sequence)  \n",
    "   - `C` = embedding size (dimension of each token's representation)\n",
    "\n",
    "2. **Generate Q, K, V Matrices**: \n",
    "   - Apply linear transformation: `kqv = self.linear_layer(x)`\n",
    "   - Output shape: `(B, T, 3 * embed_size)`\n",
    "   - Split into three equal parts: `k, q, v = kqv.split(embed_size, dim=2)`\n",
    "   - Each tensor now has shape: `(B, T, embed_size)`\n",
    "\n",
    "3. **Reshape for Multi-Head Attention**: Transform each tensor to separate the attention heads:\n",
    "   - Reshape `k, q, v` tensors: `(B, T, embed_size)` â†’ `(B, T, num_heads, head_size)`\n",
    "   - Transpose dimensions: `(B, T, num_heads, head_size)` â†’ `(B, num_heads, T, head_size)`\n",
    "   - This allows processing all heads in parallel\n",
    "\n",
    "4. **Compute Attention Scores**: Calculate how much each position should attend to others:\n",
    "   - Matrix multiply queries and keys: `scores = q @ k.transpose(-2, -1)`\n",
    "   - Scale by head dimension: `scores = scores / sqrt(head_size)`\n",
    "   - Result shape: `(B, num_heads, T, T)` - each position has scores for all positions\n",
    "\n",
    "5. **Apply Causal Masking**: Prevent attending to future positions:\n",
    "   - Set future positions to negative infinity: `scores.masked_fill(self.mask == 0, float('-inf'))`\n",
    "   - This ensures softmax gives zero probability to future positions\n",
    "\n",
    "6. **Convert to Probabilities**: Apply softmax to get attention weights:\n",
    "   - `attention_weights = softmax(scores, dim=-1)`\n",
    "   - Shape remains: `(B, num_heads, T, T)`\n",
    "   - Each row now sums to 1.0\n",
    "\n",
    "7. **Apply Attention to Values**: Use weights to combine value vectors:\n",
    "   - `output = attention_weights @ v`\n",
    "   - Output shape: `(B, num_heads, T, head_size)`\n",
    "\n",
    "8. **Concatenate Multi-Head Results**: Combine all attention heads:\n",
    "   - Transpose back: `(B, num_heads, T, head_size)` â†’ `(B, T, num_heads, head_size)`\n",
    "   - Reshape to flatten heads: `(B, T, num_heads, head_size)` â†’ `(B, T, embed_size)`\n",
    "   - Ensure tensor is contiguous in memory before reshaping\n",
    "\n",
    "9. **Final Output Projection**: Apply final linear transformation:\n",
    "   - `final_output = self.output_projection(concatenated_output)`\n",
    "   - Input and output both have shape: `(B, T, embed_size)`\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "**Dimension Tracking**: Always verify your tensor shapes at each step. The most common bugs come from incorrect reshaping or matrix multiplication dimensions.\n",
    "\n",
    "**Memory Layout**: Use `.contiguous()` before reshaping operations to ensure proper memory layout.\n",
    "\n",
    "**Masking Strategy**: Pre-compute your causal mask during initialization and store it as a buffer to avoid recomputing it every forward pass.\n",
    "\n",
    "**Scaling Factor**: The `1/sqrt(head_size)` scaling prevents attention scores from becoming too large, which would make gradients vanish after softmax.\n",
    "\n",
    "**Efficient Computation**: Modern frameworks often provide optimized attention functions (like `scaled_dot_product_attention`) that handle steps 4-7 efficiently, but understanding the manual implementation helps you debug and customize the mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        assert config.embed_size % config.num_heads == 0\n",
    "        \n",
    "        # self-attention values (key, query, value). pay attention that head_size * num_heads must be equal to embed_size\n",
    "        self.c_atten = nn.Linear(config.embed_size, 3*config.embed_size) # multiply by 3 to get k, q, v\n",
    "        \n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.embed_size, config.embed_size)\n",
    "        \n",
    "        self.embed_size = config.embed_size\n",
    "        self.num_heads = config.num_heads\n",
    "\n",
    "        mask = torch.tril(torch.ones(config.block_size, config.block_size)) # Lower triangular mask for causal attention\n",
    "        mask = mask.view(1, 1, config.block_size, config.block_size) # B, 1, T, T\n",
    "        self.register_buffer('mask', mask)  # register_buffer allows the mask to be part of the model state but not a parameter to optimize\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #  after each line print the shape of x\n",
    "        \"\"\"        Forward pass for the self-attention block.\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, T, C) where B is batch size, T is sequence length, C is embedding size\n",
    "        Returns:\n",
    "            out: Output tensor of shape (B, T, C) after self-attention and projection\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape # Batch, sequence length, embed_size\n",
    "        ## TODO: Implement the forward pass of the masked multi head self-attention block\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da9c70",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "The transformer block is a single layer containing two main components: **Multi-Head Attention** followed by **MLP (Multi-Layer Perceptron)**. This layer design gets stacked multiple times to create the full transformer.\n",
    "\n",
    "### Layer Structure\n",
    "\n",
    "**Multi-Head Attention Layer**: Allows each position to gather information from other positions in the sequence. Multiple attention heads work in parallel, each potentially focusing on different types of relationships.\n",
    "\n",
    "**MLP Layer**: Processes each position independently through a feed-forward network, applying learned transformations to refine the representations.\n",
    "\n",
    "### Residual Connections - The Critical Bridge\n",
    "\n",
    "**What Residual Connections Are**: Instead of just passing the output forward, you add the original input back to the output: `output = input + layer(input)`. This creates a direct path for information to flow unchanged.\n",
    "\n",
    "**Why Residual Connections Are Essential**:\n",
    "- **Gradient Flow**: In deep networks, gradients can vanish as they backpropagate through many layers. Residuals provide direct gradient highways.\n",
    "- **Identity Preservation**: The network can learn to keep useful information unchanged by making the layer output close to zero.\n",
    "- **Easier Learning**: The layer only needs to learn the \"difference\" or \"refinement\" rather than completely new representations.\n",
    "- **Stable Training**: Without residuals, very deep transformers become difficult or impossible to train.\n",
    "\n",
    "### Layer-by-Layer Processing\n",
    "\n",
    "**First Layer - Multi-Head Attention**:\n",
    "1. Normalize the input\n",
    "2. Apply multi-head attention to capture inter-position relationships  \n",
    "3. Add the original input back (residual connection)\n",
    "\n",
    "**Second Layer - MLP**:\n",
    "1. Normalize the current representations\n",
    "2. Apply feed-forward transformations to each position\n",
    "3. Add the pre-MLP representations back (residual connection)\n",
    "\n",
    "The residual connections ensure that even in a 50-layer transformer, information and gradients can flow directly from the first layer to the last, enabling the training of very deep and powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.embed_size)\n",
    "        self.ln2 = nn.LayerNorm(config.embed_size)\n",
    "        self.mlp = Mlp(config)\n",
    "        self.self_attn = SelfAttentionBlock(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## TODO: Implement the forward pass of the block\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1274aa",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "\n",
    "The `GPT2` class represents the complete transformer model - a decoder-only architecture that processes text and predicts the next token. This is the full model that combines all the components you've built.\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "**Embedding Layers**: Convert discrete tokens into continuous vector representations that the neural network can process.\n",
    "\n",
    "**Transformer Stack**: Multiple transformer blocks stacked together, each refining the representations further.\n",
    "\n",
    "**Output Layer**: Converts the final representations back to vocabulary predictions for next-token generation.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "**Token Embedding (`wte`)**: Maps each vocabulary word to a learned vector representation. Transforms token IDs into dense embeddings of size `embed_size`.\n",
    "\n",
    "**Position Embedding (`wpe`)**: Adds positional information since attention doesn't inherently understand word order. Each position gets its own learned embedding.\n",
    "\n",
    "**Transformer Blocks**: Stack of identical blocks (typically 12-48 layers) that progressively refine the representations through attention and feed-forward processing.\n",
    "\n",
    "**Final Layer Norm (`ln_f`)**: Normalizes the output from all transformer blocks before making predictions.\n",
    "\n",
    "**Language Model Head (`lm_head`)**: Linear layer that converts final embeddings back to vocabulary logits for next-token prediction.\n",
    "\n",
    "### Forward Pass Flow\n",
    "\n",
    "**Input Processing**:\n",
    "1. Convert token IDs to embeddings\n",
    "2. Add positional embeddings to provide sequence order information\n",
    "3. Combine token and position embeddings element-wise\n",
    "\n",
    "**Transformer Processing**:\n",
    "- Pass through each transformer block sequentially\n",
    "- Each block applies attention and MLP with residual connections\n",
    "- Representations become increasingly sophisticated through the stack\n",
    "\n",
    "**Output Generation**:\n",
    "1. Apply final normalization to stabilize outputs\n",
    "2. Project to vocabulary size using the language model head\n",
    "3. Compute loss against targets if provided for training\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "**Weight Sharing**: The token embedding and output projection share the same weights, reducing parameters and creating symmetry between input and output representations.\n",
    "\n",
    "**Autoregressive Training**: During training, the model predicts each position based only on previous positions, learning the sequential nature of language.\n",
    "\n",
    "**Next-Token Prediction**: The fundamental task is predicting the probability distribution over the vocabulary for what token should come next at each position.\n",
    "\n",
    "This architecture enables the model to generate coherent text by learning statistical patterns in language through the simple but powerful objective of next-token prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.embed_size), # token embeddings\n",
    "            wpe = nn.Embedding(config.max_len, config.embed_size), # positional embeddings\n",
    "            blocks = nn.ModuleList([Block(config) for _ in range(config.num_layers)]), # transformer blocks\n",
    "            ln_f = nn.LayerNorm(config.embed_size)\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.embed_size, config.vocab_size) # output layer for next token prediction\n",
    "\n",
    "        # sharing weights between token embedding and output layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight # this allows the model to use the same embeddings for input and output tokens\n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        \"\"\" Forward pass of the GPT-2 model.\n",
    "        Args:            idx: Input tensor of shape (B, T) where B is batch size and T is sequence length\n",
    "            target: Optional target tensor of shape (B, T) for computing loss\n",
    "        Returns:            logits: Output tensor of shape (B, T, vocab_size) containing logits for next token\n",
    "            loss: Optional scalar loss value if target is provided\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Input tokens length {T} exceeds maximum length {self.config.block_size}\"\n",
    "        \n",
    "        # TODO: Implement the forward pass of the GPT-2 model. \n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed0f77",
   "metadata": {},
   "source": [
    "## Text Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030245e",
   "metadata": {},
   "source": [
    "### Sampling Strategies\n",
    "\n",
    "How we convert these probabilities into actual text generation:\n",
    "\n",
    "#### 1. Greedy Decoding\n",
    "```python\n",
    "# Always pick the highest probability token\n",
    "def greedy_sample(logits):\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Result: Deterministic but potentially repetitive\n",
    "```\n",
    "\n",
    "#### 2. Temperature Sampling\n",
    "```python\n",
    "# Control randomness with temperature\n",
    "def temperature_sample(logits, temperature=0.8):\n",
    "    # Higher temperature = more random\n",
    "    # Lower temperature = more focused\n",
    "    scaled_logits = logits / temperature\n",
    "    probs = torch.softmax(scaled_logits, dim=-1)\n",
    "    return torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "# Temperature effects:\n",
    "# T=0.1: \"To be or not to be, that is the question\"  (very predictable)\n",
    "# T=0.8: \"To be or not to be, that troubles the mind\"  (creative but coherent)\n",
    "# T=2.0: \"To be or not to fish, purple dreams loudly\"  (too random)\n",
    "```\n",
    "\n",
    "#### 3. Top-k Sampling\n",
    "```python\n",
    "# Only consider the k most likely tokens\n",
    "def top_k_sample(logits, k=50):\n",
    "    values, indices = torch.topk(logits, k)\n",
    "    probs = torch.softmax(values, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    return indices[next_token]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078605dd",
   "metadata": {},
   "source": [
    "Implement a top-k sampling  function to generate text from the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model, tokenizer, k=10):\n",
    "    \"\"\"    Create a text generation pipeline for the given model and tokenizer.\n",
    "    Args:\n",
    "        model: The trained model instance\n",
    "        tokenizer: The tokenizer instance\n",
    "        k: Number of top-k tokens to sample from at each step (default: 10)\n",
    "    Returns:A function that takes a prompt and generates text\"\"\"\n",
    "    # step-by-step explanation of the pipeline function:\n",
    "    \"\"\"\n",
    "    1. Set the model to evaluation mode\n",
    "    2. Encode the input prompt using the tokenizer\n",
    "    3. Convert the encoded prompt to a tensor and move it to the model's device\n",
    "    4. While the length of the generated sequence is less than max_len:\n",
    "        a. Get the model's logits for the last token in the sequence\n",
    "        b. Extract the logits for the last token and apply softmax to get probabilities\n",
    "        v. Use top-k sampling to select the next token based on probabilities\n",
    "        d. Append the selected token to the sequence\n",
    "    5. Decode the generated sequence back to text using the tokenizer\n",
    "    6. Return the generated text\n",
    "    \"\"\"\n",
    "    def generate(prompt, max_len=1024):\n",
    "        ## TODO: Implement the pipeline function\n",
    "        pass\n",
    "    return generate\n",
    "\n",
    "\n",
    "# exmple usage of the pipeline\n",
    "\"\"\"\n",
    "generate_text = pipeline(model, tokenizer)\n",
    "prompt = \"To be or not to be, that is the question: \"\n",
    "generated_text = generate_text(prompt, max_len=100)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dff197",
   "metadata": {},
   "source": [
    "### Conig the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3dd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the most minmal config class for GPT-2 (Takes 40-60 to train). You are welcome to modify it to a bigger model or for loger but no the other way around (expet of batch size which you can reduce if you run out of memory)\n",
    "class GPT2Config:\n",
    "    def __init__(self):\n",
    "        self.embed_size = 64\n",
    "        self.num_heads = 2\n",
    "        self.num_layers = 4\n",
    "        self.vocab_size = len(tokenizer)\n",
    "        self.block_size = 256\n",
    "        self.lr = 3e-4\n",
    "        self.batch_size = 32 # use a smaller batch size if you run out of memory\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.path = 'models/model.pth'\n",
    "        self.num_epochs = 10\n",
    "\n",
    "        self.patience = 5  # Early stopping patience\n",
    "        self.grad_clip = 1.0  # Gradient clipping threshold\n",
    "        \n",
    "        # Learning rate scheduler configs\n",
    "        self.use_scheduler = True\n",
    "        self.scheduler_type = 'reduce_on_plateau'  # 'reduce_on_plateau', 'cosine', 'step'\n",
    "        self.lr_patience = 2  # For ReduceLROnPlateau\n",
    "        self.lr_factor = 0.5  # For ReduceLROnPlateau\n",
    "        self.step_size = 1  # For StepLR\n",
    "        self.gamma = 0.1  # For StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202876e",
   "metadata": {},
   "source": [
    "## Traninig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f822ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning function\n",
    "def train_model(model, train_loader, val_loader, optimizer, config=None):\n",
    "\n",
    "    # Setup\n",
    "    model.train()\n",
    "    summary_writer = SummaryWriter(log_dir='runs/shakespeare_experiment')\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Training state tracking\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = getattr(config, 'patience', 5)  # Early stopping patience\n",
    "    \n",
    "    # Loss tracking\n",
    "    train_losses = deque(maxlen=1000)\n",
    "    global_step = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Training for {config.num_epochs} epochs\")\n",
    "    print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{config.num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        with tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\") as pbar:\n",
    "            for batch_idx, (x, y) in enumerate(pbar):\n",
    "                x, y = x.to(config.device), y.to(config.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                logits, loss = model(x, y)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping (optional but recommended)\n",
    "                if hasattr(config, 'grad_clip') and config.grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track losses\n",
    "                loss_item = loss.item()\n",
    "                train_losses.append(loss_item)\n",
    "                epoch_train_losses.append(loss_item)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f\"{loss_item:.4f}\",\n",
    "                    'avg_loss': f\"{np.mean(train_losses):.4f}\",\n",
    "                    'lr': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "                })\n",
    "                \n",
    "                # Log to tensorboard every N steps\n",
    "                if global_step % 10 == 0:  # Log every 10 steps\n",
    "                    summary_writer.add_scalar('Loss/train_step', loss_item, global_step)\n",
    "                    summary_writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "                \n",
    "                global_step += 1\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\", leave=False) as val_pbar:\n",
    "                for x, y in val_loader:\n",
    "                    x, y = x.to(config.device), y.to(config.device)\n",
    "                    logits, loss = model(x, y)\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_pbar.set_postfix({'val_loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        summary_writer.add_scalar('Loss/train_epoch', avg_train_loss, epoch)\n",
    "        summary_writer.add_scalar('Loss/validation_epoch', avg_val_loss, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        \n",
    "        # Model checkpointing\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'global_step': global_step\n",
    "        }\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(checkpoint, os.path.join('models', 'best_model.pt'))\n",
    "            print(f\"New best model saved! Val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        torch.save(checkpoint, os.path.join('models', f'checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {patience} epochs without improvement\")\n",
    "            break\n",
    "        \n",
    "        # Save latest model using your custom save method\n",
    "        if hasattr(model, 'save_model'):\n",
    "            model.save_model(config.path)\n",
    "    \n",
    "    # Final logging\n",
    "    summary_writer.close()\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': avg_train_loss,\n",
    "        'final_val_loss': avg_val_loss,\n",
    "        'epochs_trained': epoch + 1\n",
    "    }\n",
    "\n",
    "def estimate_loss(model, eval_iters, data_loaders={}):\n",
    "    \"\"\"        Estimate the loss of the model on the given splits.\n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        eval_iters: Number of iterations to average the loss over\n",
    "        splits: List of dataset splits to evaluate (e.g., 'train', 'validation')\n",
    "        data_loader: DataLoader instance for fetching batches\n",
    "    Returns:\n",
    "        out: Dictionary with average loss for each split\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split, dl in data_loaders.items():\n",
    "            losses = 0\n",
    "            for i in range(eval_iters):\n",
    "                x, y = next(iter(dl))\n",
    "                x, y = x.to(model.config.device), y.to(model.config.device)\n",
    "                _, loss = model(x, y)\n",
    "                losses += loss.item()\n",
    "            out[split] = losses / eval_iters\n",
    "        model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bfaee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init config and datasets\n",
    "con = GPT2Config()\n",
    "train_dataset = ShakespeareDataset(train_data, con.block_size)\n",
    "val_dataset = ShakespeareDataset(val_data, con.block_size)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=con.batch_size,\n",
    "    shuffle=shuffle_train,\n",
    "    num_workers=4,\n",
    "    drop_last=True  # Drop last incomplete batch for consistent batch sizes\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=con.batch_size,\n",
    "    shuffle=False,  # Don't shuffle validation data\n",
    "    num_workers=4,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "\n",
    "model = GPT2(con)\n",
    "model.to(con.device)\n",
    "print(f\"device: {con.device}\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=con.lr)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an optimizer \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=con.lr, weight_decay=0.01)\n",
    "\n",
    "# train the model\n",
    "results = train_model(model, train_loader, val_loader, optimizer, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89517e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(model, tokenizer, k=10)\n",
    "print(generator('to be or not to be', max_len=1024)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
